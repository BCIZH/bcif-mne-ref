# scikit-learn 无监督学习模块详解

## 1. 简介

无监督学习从未标记的数据中发现模式和结构。scikit-learn 提供了聚类、降维、密度估计等多种无监督学习算法。

### 1.1 无监督学习模块概览

| 模块 | 任务类型 | 主要算法 |
|-----|---------|---------|
| `cluster` | 聚类 | KMeans、DBSCAN、层次聚类、谱聚类 |
| `decomposition` | 降维/矩阵分解 | PCA、NMF、ICA、因子分析 |
| `manifold` | 流形学习 | t-SNE、Isomap、MDS、LLE |
| `mixture` | 混合模型 | 高斯混合模型 |
| `covariance` | 协方差估计 | 经验协方差、收缩估计、GraphicalLasso |

## 2. cluster - 聚类

位置：`sklearn/cluster/`

### 2.1 核心算法

#### 2.1.1 KMeans

**最常用的聚类算法**，基于距离的分区聚类：

$$\min_{C} \sum_{i=1}^n \min_{\mu_j \in C} ||x_i - \mu_j||^2$$

```python
from sklearn.cluster import KMeans

kmeans = KMeans(
    n_clusters=3,            # 簇的数量
    init='k-means++',        # 初始化方法
    n_init=10,               # 运行次数
    max_iter=300,
    random_state=42
)

labels = kmeans.fit_predict(X)
centers = kmeans.cluster_centers_

# 惯性（簇内平方和）
print(kmeans.inertia_)
```

**算法流程**:
1. 随机初始化 k 个中心点
2. 分配：每个点分配到最近的中心
3. 更新：重新计算每个簇的中心
4. 重复 2-3 直到收敛

**变体**:
- `MiniBatchKMeans`: 使用小批量数据，更快但略微不准确

```python
from sklearn.cluster import MiniBatchKMeans

kmeans = MiniBatchKMeans(n_clusters=3, batch_size=100)
kmeans.fit(X)
```

#### 2.1.2 DBSCAN

**基于密度的聚类**，可以发现任意形状的簇：

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(
    eps=0.5,                 # 邻域半径
    min_samples=5,           # 核心点的最小邻居数
    metric='euclidean'
)

labels = dbscan.fit_predict(X)

# -1 表示噪声点
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)
```

**优点**:
- 不需要指定簇的数量
- 可以发现任意形状的簇
- 可以识别噪声点

**核心概念**:
- **核心点**: 邻域内至少有 min_samples 个点
- **边界点**: 不是核心点，但在某个核心点的邻域内
- **噪声点**: 既不是核心点也不是边界点

#### 2.1.3 层次聚类 (AgglomerativeClustering)

**自底向上的聚类方法**：

```python
from sklearn.cluster import AgglomerativeClustering

agg = AgglomerativeClustering(
    n_clusters=3,
    linkage='ward',          # 'ward', 'complete', 'average', 'single'
    metric='euclidean'
)

labels = agg.fit_predict(X)
```

**链接方法**:
- `ward`: 最小化簇内方差（默认，仅限欧几里得距离）
- `complete`: 最大距离（最远点）
- `average`: 平均距离
- `single`: 最小距离（最近点）

**树状图可视化**:
```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

Z = linkage(X, method='ward')
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.show()
```

#### 2.1.4 谱聚类 (SpectralClustering)

**基于图论的聚类**，适合非凸形状：

```python
from sklearn.cluster import SpectralClustering

spectral = SpectralClustering(
    n_clusters=3,
    affinity='rbf',          # 'rbf', 'nearest_neighbors', 'precomputed'
    assign_labels='kmeans',  # 'kmeans', 'discretize'
    random_state=42
)

labels = spectral.fit_predict(X)
```

**算法流程**:
1. 构建相似度图
2. 计算拉普拉斯矩阵
3. 计算特征向量
4. 在特征空间中用 KMeans 聚类

#### 2.1.5 均值漂移 (MeanShift)

**基于密度的聚类**，自动确定簇数：

```python
from sklearn.cluster import MeanShift, estimate_bandwidth

bandwidth = estimate_bandwidth(X, quantile=0.2)

ms = MeanShift(bandwidth=bandwidth)
labels = ms.fit_predict(X)

# 自动确定的簇数
n_clusters = len(ms.cluster_centers_)
```

#### 2.1.6 OPTICS

**类似 DBSCAN，但对参数更不敏感**：

```python
from sklearn.cluster import OPTICS

optics = OPTICS(
    min_samples=5,
    xi=0.05,
    min_cluster_size=0.1
)

labels = optics.fit_predict(X)
```

#### 2.1.7 Birch

**适合大数据集的层次聚类**：

```python
from sklearn.cluster import Birch

birch = Birch(
    n_clusters=3,
    threshold=0.5,           # CF Tree 的阈值
    branching_factor=50
)

labels = birch.fit_predict(X)
```

### 2.2 聚类评估

#### 2.2.1 轮廓系数 (Silhouette Score)

```python
from sklearn.metrics import silhouette_score, silhouette_samples

# 平均轮廓系数
score = silhouette_score(X, labels)

# 每个样本的轮廓系数
sample_scores = silhouette_samples(X, labels)
```

**取值范围**: [-1, 1]
- 接近 1: 聚类良好
- 接近 0: 在簇边界上
- 接近 -1: 可能分配错误

#### 2.2.2 Calinski-Harabasz 指数

```python
from sklearn.metrics import calinski_harabasz_score

score = calinski_harabasz_score(X, labels)  # 越大越好
```

#### 2.2.3 Davies-Bouldin 指数

```python
from sklearn.metrics import davies_bouldin_score

score = davies_bouldin_score(X, labels)  # 越小越好
```

#### 2.2.4 肘部法则（选择 K）

```python
import matplotlib.pyplot as plt

inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, 'bo-')
plt.xlabel('K')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()
```

## 3. decomposition - 降维与矩阵分解

位置：`sklearn/decomposition/`

### 3.1 核心算法

#### 3.1.1 PCA (主成分分析)

**最常用的降维方法**，寻找最大方差方向：

```python
from sklearn.decomposition import PCA

pca = PCA(
    n_components=2,          # 主成分数量，或方差比例（如 0.95）
    whiten=False,            # 是否白化
    random_state=42
)

X_pca = pca.fit_transform(X)

# 解释的方差比例
print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())

# 主成分（加载）
print(pca.components_)

# 逆变换
X_reconstructed = pca.inverse_transform(X_pca)
```

**选择主成分数量**:
```python
# 方法1: 保留 95% 方差
pca = PCA(n_components=0.95)

# 方法2: 碎石图
pca = PCA()
pca.fit(X)

plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
         pca.explained_variance_ratio_, 'bo-')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.show()
```

**变体**:
- `IncrementalPCA`: 增量 PCA，适合大数据
- `KernelPCA`: 核 PCA，非线性降维
- `SparsePCA`: 稀疏 PCA

```python
from sklearn.decomposition import IncrementalPCA, KernelPCA, SparsePCA

# 增量 PCA
ipca = IncrementalPCA(n_components=2, batch_size=100)
for batch in batches:
    ipca.partial_fit(batch)

# 核 PCA
kpca = KernelPCA(n_components=2, kernel='rbf')
X_kpca = kpca.fit_transform(X)

# 稀疏 PCA
spca = SparsePCA(n_components=2, alpha=1.0)
X_spca = spca.fit_transform(X)
```

#### 3.1.2 NMF (非负矩阵分解)

**要求数据和成分都非负**，适合主题建模：

$$\min_{W, H} ||X - WH||_F^2 + \alpha ||W||_1 + \alpha ||H||_1$$

```python
from sklearn.decomposition import NMF

nmf = NMF(
    n_components=10,
    init='nndsvd',           # 初始化方法
    max_iter=200,
    random_state=42
)

W = nmf.fit_transform(X)     # 文档-主题矩阵
H = nmf.components_           # 主题-词矩阵

# 重构误差
reconstruction_err = nmf.reconstruction_err_
```

**应用示例（主题建模）**:
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 文本向量化
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(documents)

# NMF 主题建模
nmf = NMF(n_components=10)
doc_topics = nmf.fit_transform(X)

# 查看主题
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(nmf.components_):
    top_words = [feature_names[i] for i in topic.argsort()[-10:]]
    print(f"Topic {topic_idx}: {', '.join(top_words)}")
```

#### 3.1.3 ICA (独立成分分析)

**寻找统计独立的成分**，用于盲源分离：

```python
from sklearn.decomposition import FastICA

ica = FastICA(
    n_components=3,
    algorithm='parallel',    # 'parallel', 'deflation'
    whiten='unit-variance',
    max_iter=200
)

S = ica.fit_transform(X)     # 独立成分
A = ica.mixing_              # 混合矩阵
```

**应用场景**:
- 信号分离（如鸡尾酒会问题）
- 脑电图分析

#### 3.1.4 因子分析 (FactorAnalysis)

**假设数据由少数潜在因子生成**：

```python
from sklearn.decomposition import FactorAnalysis

fa = FactorAnalysis(
    n_components=3,
    max_iter=1000
)

X_fa = fa.fit_transform(X)
```

#### 3.1.5 截断 SVD (TruncatedSVD)

**类似 PCA，但不中心化，适合稀疏矩阵**：

```python
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(
    n_components=100,
    algorithm='randomized'   # 'randomized', 'arpack'
)

X_svd = svd.fit_transform(X_sparse)

# LSA（潜在语义分析）
print(svd.explained_variance_ratio_)
```

#### 3.1.6 字典学习 (DictionaryLearning)

**学习稀疏编码字典**：

```python
from sklearn.decomposition import DictionaryLearning

dict_learner = DictionaryLearning(
    n_components=100,
    alpha=1.0,               # 稀疏性参数
    max_iter=100
)

code = dict_learner.fit_transform(X)
dictionary = dict_learner.components_
```

#### 3.1.7 LatentDirichletAllocation (LDA)

**主题模型**，用于文本分析：

```python
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(
    n_components=10,         # 主题数
    learning_method='online',
    max_iter=10
)

doc_topics = lda.fit_transform(X)
```

## 4. manifold - 流形学习

位置：`sklearn/manifold/`

### 4.1 核心算法

#### 4.1.1 t-SNE

**最流行的可视化降维方法**：

```python
from sklearn.manifold import TSNE

tsne = TSNE(
    n_components=2,          # 通常是 2 或 3
    perplexity=30,           # 平滑度参数（5-50）
    learning_rate='auto',
    n_iter=1000,
    random_state=42
)

X_tsne = tsne.fit_transform(X)

# 可视化
import matplotlib.pyplot as plt
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.show()
```

**注意事项**:
- 仅用于可视化，不能 transform 新数据
- 计算开销大
- 结果受随机初始化影响

#### 4.1.2 Isomap

**保持测地距离的流形学习**：

```python
from sklearn.manifold import Isomap

isomap = Isomap(
    n_neighbors=5,           # 近邻数
    n_components=2
)

X_isomap = isomap.fit_transform(X)
```

#### 4.1.3 MDS (多维缩放)

**保持样本间距离**：

```python
from sklearn.manifold import MDS

mds = MDS(
    n_components=2,
    metric=True,             # True: 度量 MDS, False: 非度量 MDS
    max_iter=300
)

X_mds = mds.fit_transform(X)
```

#### 4.1.4 局部线性嵌入 (LLE)

**假设每个点可由其邻居线性表示**：

```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(
    n_neighbors=10,
    n_components=2,
    method='standard'        # 'standard', 'modified', 'hessian', 'ltsa'
)

X_lle = lle.fit_transform(X)
```

## 5. mixture - 混合模型

位置：`sklearn/mixture/`

### 5.1 高斯混合模型 (GaussianMixture)

**软聚类，假设数据由多个高斯分布生成**：

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(
    n_components=3,
    covariance_type='full',  # 'full', 'tied', 'diag', 'spherical'
    max_iter=100,
    random_state=42
)

gmm.fit(X)

# 预测簇标签
labels = gmm.predict(X)

# 预测概率
proba = gmm.predict_proba(X)

# 生成新样本
X_new, y_new = gmm.sample(100)

# 模型选择（BIC/AIC）
print(gmm.bic(X))
print(gmm.aic(X))
```

**协方差类型**:
- `full`: 每个成分有独立的协方差矩阵
- `tied`: 所有成分共享协方差矩阵
- `diag`: 对角协方差矩阵
- `spherical`: 球形协方差矩阵

### 5.2 贝叶斯高斯混合 (BayesianGaussianMixture)

**自动确定成分数量**：

```python
from sklearn.mixture import BayesianGaussianMixture

bgm = BayesianGaussianMixture(
    n_components=10,         # 最大成分数
    covariance_type='full',
    weight_concentration_prior_type='dirichlet_process'
)

bgm.fit(X)
labels = bgm.predict(X)

# 实际使用的成分数（权重 > 阈值）
weights = bgm.weights_
n_active = (weights > 0.01).sum()
```

## 6. covariance - 协方差估计

位置：`sklearn/covariance/`

### 6.1 核心算法

#### 6.1.1 经验协方差 (EmpiricalCovariance)

```python
from sklearn.covariance import EmpiricalCovariance

cov = EmpiricalCovariance()
cov.fit(X)

# 协方差矩阵
print(cov.covariance_)

# Mahalanobis 距离
distances = cov.mahalanobis(X)
```

#### 6.1.2 收缩协方差 (ShrunkCovariance)

**在样本数少时更稳定**：

```python
from sklearn.covariance import ShrunkCovariance

shrunk_cov = ShrunkCovariance(shrinkage=0.1)
shrunk_cov.fit(X)
```

#### 6.1.3 Ledoit-Wolf 收缩

**自动选择收缩参数**：

```python
from sklearn.covariance import LedoitWolf

lw = LedoitWolf()
lw.fit(X)
print(lw.shrinkage_)  # 最优收缩参数
```

#### 6.1.4 GraphicalLasso

**稀疏逆协方差估计**：

```python
from sklearn.covariance import GraphicalLasso

gl = GraphicalLasso(alpha=0.01)
gl.fit(X)

# 精度矩阵（逆协方差）
print(gl.precision_)
```

## 7. 降维方法对比

### 7.1 算法特点

| 算法 | 类型 | 线性/非线性 | 可逆 | 适用场景 |
|-----|------|-----------|------|---------|
| PCA | 方差最大化 | 线性 | 是 | 通用降维、去噪 |
| KernelPCA | 方差最大化 | 非线性 | 否 | 非线性数据 |
| NMF | 非负分解 | 线性 | 近似 | 主题建模、图像 |
| ICA | 独立性 | 线性 | 是 | 信号分离 |
| t-SNE | 保持局部结构 | 非线性 | 否 | 可视化 |
| Isomap | 保持测地距离 | 非线性 | 否 | 流形学习 |
| LLE | 局部线性 | 非线性 | 否 | 流形学习 |
| MDS | 保持距离 | 非线性 | 否 | 可视化 |

### 7.2 选择指南

```
需要可视化（2D/3D）？
├─ 是 → t-SNE, UMAP
└─ 否 → 继续

数据非负（如图像、文本）？
├─ 是 → NMF
└─ 否 → 继续

需要逆变换？
├─ 是 → PCA, ICA
└─ 否 → 继续

数据在低维流形上？
├─ 是 → Isomap, LLE
└─ 否 → PCA

需要解释性？
└─ PCA（主成分可解释）
```

## 8. 聚类算法对比

### 8.1 算法特点

| 算法 | 簇形状 | 需要 K | 可扩展性 | 处理噪声 |
|-----|-------|-------|---------|---------|
| KMeans | 凸形 | 是 | 好 | 差 |
| DBSCAN | 任意 | 否 | 中 | 好 |
| Agglomerative | 任意 | 是 | 差 | 中 |
| SpectralClustering | 任意 | 是 | 差 | 中 |
| MeanShift | 任意 | 否 | 差 | 好 |
| GaussianMixture | 椭圆 | 是 | 中 | 中 |

### 8.2 选择指南

```
数据量大（> 10000）？
├─ 是 → KMeans, MiniBatchKMeans, Birch
└─ 否 → 继续

知道簇的数量？
├─ 否 → DBSCAN, MeanShift
└─ 是 → 继续

簇的形状复杂（非凸）？
├─ 是 → DBSCAN, SpectralClustering
└─ 否 → KMeans

需要概率/软聚类？
└─ GaussianMixture

需要层次结构？
└─ AgglomerativeClustering
```

## 9. 实用示例

### 9.1 图像压缩（KMeans + 颜色量化）

```python
from sklearn.cluster import KMeans
import numpy as np

# 图像形状: (height, width, 3)
image = ...  # 加载图像
h, w, c = image.shape

# 重塑为 (n_pixels, 3)
pixels = image.reshape(-1, 3)

# 聚类颜色
kmeans = KMeans(n_clusters=16, random_state=42)
labels = kmeans.fit_predict(pixels)

# 用聚类中心替换颜色
compressed = kmeans.cluster_centers_[labels]
compressed_image = compressed.reshape(h, w, c)
```

### 9.2 文档聚类（TF-IDF + KMeans）

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# 文本向量化
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(documents)

# 聚类
kmeans = KMeans(n_clusters=5, random_state=42)
labels = kmeans.fit_predict(X)

# 查看每个簇的关键词
terms = vectorizer.get_feature_names_out()
for i in range(5):
    cluster_center = kmeans.cluster_centers_[i]
    top_terms = [terms[j] for j in cluster_center.argsort()[-10:]]
    print(f"Cluster {i}: {', '.join(top_terms)}")
```

### 9.3 异常检测（孤立森林 + PCA）

```python
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest

# 降维
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X)

# 异常检测
iso_forest = IsolationForest(contamination=0.1, random_state=42)
outliers = iso_forest.fit_predict(X_pca)  # -1 为异常
```

### 9.4 高维数据可视化（PCA + t-SNE）

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# 先用 PCA 降维到 50 维
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X)

# 再用 t-SNE 降到 2 维
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_pca)

# 可视化
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.show()
```

## 10. 总结

无监督学习的关键挑战是评估和验证，因为没有标签：

1. **聚类**: 使用轮廓系数、Calinski-Harabasz 指数等内部指标
2. **降维**: 检查重构误差、解释方差比例
3. **可视化**: t-SNE 最流行，但要注意参数影响
4. **实际应用**: 通常结合多种方法（如 PCA + KMeans）

选择合适的算法需要：
- 理解数据特性（大小、维度、分布）
- 明确目标（聚类、降维、可视化）
- 验证结果的稳定性和可解释性

---

**相关文档**:
- [01_scikit-learn架构总览.md](01_scikit-learn架构总览.md)
- [03_监督学习模块.md](03_监督学习模块.md)
- [05_模型选择与评估.md](05_模型选择与评估.md)
- [06_数据预处理与工程.md](06_数据预处理与工程.md)
