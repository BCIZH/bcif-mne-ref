# scikit-learn 监督学习模块详解

## 1. 简介

监督学习是机器学习的核心任务，包括分类（预测离散标签）和回归（预测连续值）。scikit-learn 提供了丰富的监督学习算法。

### 1.1 监督学习模块概览

| 模块 | 类型 | 主要算法 |
|-----|------|---------|
| `linear_model` | 分类/回归 | 线性回归、岭回归、Lasso、逻辑回归、SGD |
| `tree` | 分类/回归 | 决策树 |
| `ensemble` | 分类/回归 | 随机森林、梯度提升、AdaBoost |
| `svm` | 分类/回归 | 支持向量机 |
| `neural_network` | 分类/回归 | 多层感知器 |
| `neighbors` | 分类/回归 | K近邻 |
| `gaussian_process` | 分类/回归 | 高斯过程 |
| `naive_bayes` | 分类 | 朴素贝叶斯 |
| `discriminant_analysis` | 分类 | LDA、QDA |

## 2. linear_model - 线性模型

位置：`sklearn/linear_model/`

### 2.1 模块结构

```
linear_model/
├── _base.py                  # 线性模型基类
├── _ridge.py                 # 岭回归
├── _coordinate_descent.py    # Lasso、ElasticNet
├── _logistic.py              # 逻辑回归
├── _stochastic_gradient.py   # SGD
├── _least_angle.py           # LARS
├── _omp.py                   # 正交匹配追踪
├── _ransac.py                # RANSAC
├── _theil_sen.py             # Theil-Sen 回归
├── _huber.py                 # Huber 回归
├── _quantile.py              # 分位数回归
├── _passive_aggressive.py    # Passive Aggressive
├── _perceptron.py            # 感知器
├── _bayes.py                 # 贝叶斯回归
├── _glm/                     # 广义线性模型
│   ├── _glm.py
│   └── glm.py
└── tests/
```

### 2.2 核心算法

#### 2.2.1 线性回归 (LinearRegression)

**最简单的线性模型**，使用最小二乘法拟合：

$$\min_w ||Xw - y||_2^2$$

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 学习到的参数
print(model.coef_)       # 系数
print(model.intercept_)  # 截距
```

**实现**:
- 使用 `scipy.linalg.lstsq` 求解
- 无正则化
- 不适合高维数据或共线性问题

#### 2.2.2 岭回归 (Ridge)

**L2 正则化**线性回归：

$$\min_w ||Xw - y||_2^2 + \alpha||w||_2^2$$

```python
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)  # alpha 控制正则化强度
model.fit(X_train, y_train)
```

**特点**:
- 防止过拟合
- 处理共线性
- 所有特征都保留（系数不为 0）
- 有闭式解

**变体**:
- `RidgeCV`: 自动选择最佳 alpha（交叉验证）
- `RidgeClassifier`: 用于分类的岭回归

#### 2.2.3 Lasso

**L1 正则化**线性回归：

$$\min_w \frac{1}{2n}||Xw - y||_2^2 + \alpha||w||_1$$

```python
from sklearn.linear_model import Lasso

model = Lasso(alpha=0.1)
model.fit(X_train, y_train)

# 许多系数会变成 0（特征选择）
print(np.sum(model.coef_ == 0))  # 被移除的特征数
```

**特点**:
- 特征选择（稀疏解）
- 某些系数会精确为 0
- 使用坐标下降算法（Cython 优化）

**变体**:
- `LassoCV`: 自动选择最佳 alpha
- `LassoLars`: 使用 LARS 算法
- `MultiTaskLasso`: 多任务学习

#### 2.2.4 ElasticNet

**L1 + L2 混合正则化**：

$$\min_w \frac{1}{2n}||Xw - y||_2^2 + \alpha \rho||w||_1 + \frac{\alpha(1-\rho)}{2}||w||_2^2$$

```python
from sklearn.linear_model import ElasticNet

model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio 控制 L1/L2 比例
model.fit(X_train, y_train)
```

**特点**:
- 结合 Ridge 和 Lasso 的优点
- `l1_ratio=1` 等同于 Lasso
- `l1_ratio=0` 等同于 Ridge

#### 2.2.5 逻辑回归 (LogisticRegression)

**分类算法**，优化逻辑损失：

$$\min_w C\sum_{i=1}^n \log(\exp(-y_i(X_i^T w + b)) + 1) + ||w||_2^2$$

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(
    penalty='l2',      # 'l1', 'l2', 'elasticnet', 'none'
    C=1.0,             # 正则化强度的倒数
    solver='lbfgs',    # 优化算法
    max_iter=100
)
clf.fit(X_train, y_train)

# 预测
y_pred = clf.predict(X_test)
y_proba = clf.predict_proba(X_test)
```

**求解器（solver）**:
- `lbfgs`: L-BFGS（默认，适合中小数据）
- `liblinear`: 坐标下降（适合小数据）
- `sag`/`saga`: 随机平均梯度（适合大数据）
- `newton-cg`: 牛顿法

**多分类策略**:
- One-vs-Rest (OvR): 默认
- Multinomial: 多项式逻辑回归

#### 2.2.6 随机梯度下降 (SGDClassifier/SGDRegressor)

**在线学习算法**，适合大规模数据：

```python
from sklearn.linear_model import SGDClassifier

clf = SGDClassifier(
    loss='hinge',         # 'hinge', 'log_loss', 'modified_huber', ...
    penalty='l2',         # 正则化
    alpha=0.0001,
    max_iter=1000,
    learning_rate='optimal'
)

# 支持增量学习
for X_batch, y_batch in batches:
    clf.partial_fit(X_batch, y_batch, classes=np.unique(y))
```

**损失函数**:
- `hinge`: SVM
- `log_loss`: 逻辑回归
- `modified_huber`: 平滑的 hinge
- `squared_error`: 线性回归

#### 2.2.7 其他线性模型

**广义线性模型 (GLM)**:
```python
from sklearn.linear_model import PoissonRegressor, GammaRegressor, TweedieRegressor

# 泊松回归（计数数据）
model = PoissonRegressor()

# Gamma 回归（严格正数据）
model = GammaRegressor()

# Tweedie 回归（复合泊松-Gamma）
model = TweedieRegressor(power=1.5)
```

**鲁棒回归**:
```python
from sklearn.linear_model import HuberRegressor, RANSACRegressor, TheilSenRegressor

# Huber 回归（对异常值鲁棒）
model = HuberRegressor(epsilon=1.35)

# RANSAC（随机样本一致性）
model = RANSACRegressor()

# Theil-Sen 回归（中位数估计）
model = TheilSenRegressor()
```

**其他**:
```python
# 贝叶斯岭回归
from sklearn.linear_model import BayesianRidge, ARDRegression

# 正交匹配追踪
from sklearn.linear_model import OrthogonalMatchingPursuit

# Passive Aggressive
from sklearn.linear_model import PassiveAggressiveClassifier

# 感知器
from sklearn.linear_model import Perceptron
```

### 2.3 关键实现细节

#### 2.3.1 坐标下降算法

文件：`_cd_fast.pyx` (Cython)

用于 Lasso、ElasticNet 的高效优化。

#### 2.3.2 随机平均梯度 (SAG/SAGA)

文件：`_sag.py`, `_sag_fast.pyx.tp`

适合大规模数据的优化算法。

## 3. tree - 决策树

位置：`sklearn/tree/`

### 3.1 模块结构

```
tree/
├── _classes.py        # 决策树分类器和回归器
├── _tree.pyx          # 树结构（Cython）
├── _splitter.pyx      # 分裂策略
├── _criterion.pyx     # 分裂准则
├── _export.py         # 树的导出和可视化
└── tests/
```

### 3.2 核心算法

#### 3.2.1 决策树分类器 (DecisionTreeClassifier)

```python
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(
    criterion='gini',        # 'gini' 或 'entropy'
    max_depth=None,          # 最大深度
    min_samples_split=2,     # 分裂所需的最小样本数
    min_samples_leaf=1,      # 叶子节点的最小样本数
    max_features=None,       # 寻找最佳分裂时考虑的特征数
    random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# 特征重要性
print(clf.feature_importances_)

# 树结构
print(clf.tree_)
```

**分裂准则**:
- **Gini 不纯度**: $Gini(t) = 1 - \sum_{i=1}^C p_i^2$
- **信息熵**: $H(t) = -\sum_{i=1}^C p_i \log_2(p_i)$

#### 3.2.2 决策树回归器 (DecisionTreeRegressor)

```python
from sklearn.tree import DecisionTreeRegressor

reg = DecisionTreeRegressor(
    criterion='squared_error',  # 'squared_error', 'friedman_mse', 'absolute_error'
    max_depth=5
)

reg.fit(X_train, y_train)
```

**分裂准则**:
- `squared_error`: MSE
- `friedman_mse`: Friedman MSE（梯度提升优化）
- `absolute_error`: MAE

### 3.3 树的可视化

```python
from sklearn.tree import plot_tree, export_text
import matplotlib.pyplot as plt

# 绘制树
plt.figure(figsize=(20, 10))
plot_tree(clf, filled=True, feature_names=feature_names, class_names=class_names)
plt.show()

# 文本表示
tree_rules = export_text(clf, feature_names=feature_names)
print(tree_rules)
```

### 3.4 关键实现

#### 3.4.1 CART 算法

scikit-learn 使用优化的 CART（分类和回归树）算法：

1. 寻找最佳分裂点（贪心）
2. 递归构建子树
3. 剪枝（通过超参数控制）

#### 3.4.2 Cython 优化

核心算法在 Cython 中实现：
- `_tree.pyx`: 树数据结构
- `_splitter.pyx`: 分裂逻辑
- `_criterion.pyx`: 准则计算

## 4. ensemble - 集成学习

位置：`sklearn/ensemble/`

### 4.1 模块结构

```
ensemble/
├── _bagging.py                    # Bagging
├── _forest.py                     # 随机森林
├── _gb.py                         # 梯度提升（传统）
├── _hist_gradient_boosting/       # 直方图梯度提升
│   ├── gradient_boosting.py
│   ├── binning.py
│   └── ...
├── _iforest.py                    # 孤立森林
├── _voting.py                     # 投票
├── _stacking.py                   # 堆叠
├── _weight_boosting.py            # AdaBoost
└── tests/
```

### 4.2 核心算法

#### 4.2.1 随机森林 (RandomForest)

**Bagging + 决策树 + 特征随机性**：

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

clf = RandomForestClassifier(
    n_estimators=100,        # 树的数量
    max_depth=None,          # 树的最大深度
    max_features='sqrt',     # 每次分裂考虑的特征数
    min_samples_split=2,
    bootstrap=True,          # 是否使用 bootstrap 采样
    n_jobs=-1,               # 并行数
    random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# 特征重要性（平均）
print(clf.feature_importances_)

# 访问单个树
print(clf.estimators_[0])
```

**优点**:
- 高准确率
- 不易过拟合
- 特征重要性
- 并行训练

#### 4.2.2 梯度提升 (GradientBoosting)

**顺序集成，每棵树修正前面树的错误**：

```python
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

clf = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,       # 学习率
    max_depth=3,             # 树深度（通常较浅）
    subsample=1.0,           # 子采样比例
    min_samples_split=2
)

clf.fit(X_train, y_train)
```

**算法流程**:
1. 初始化 $F_0(x) = \arg\min_c \sum_{i=1}^n L(y_i, c)$
2. 对 $m = 1$ 到 $M$:
   - 计算伪残差：$r_{im} = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}$
   - 拟合树：$h_m(x)$ 拟合 $r_m$
   - 更新：$F_m(x) = F_{m-1}(x) + \nu h_m(x)$

#### 4.2.3 直方图梯度提升 (HistGradientBoosting)

**更快的梯度提升实现**（类似 LightGBM）：

```python
from sklearn.ensemble import HistGradientBoostingClassifier

clf = HistGradientBoostingClassifier(
    max_iter=100,            # 迭代次数（代替 n_estimators）
    learning_rate=0.1,
    max_depth=None,
    max_bins=255,            # 直方图的箱数
    early_stopping=True      # 早停
)

clf.fit(X_train, y_train)
```

**优点**:
- 比 GradientBoosting 快得多
- 自动处理缺失值
- 支持类别特征
- 内置早停

#### 4.2.4 AdaBoost

**自适应提升**，调整样本权重：

```python
from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # 弱学习器
    n_estimators=50,
    learning_rate=1.0
)

clf.fit(X_train, y_train)
```

#### 4.2.5 Bagging

**Bootstrap 聚合**：

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

clf = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_samples=1.0,         # 每个基估计器的样本比例
    max_features=1.0,        # 每个基估计器的特征比例
    bootstrap=True           # 是否 bootstrap 采样
)

clf.fit(X_train, y_train)
```

#### 4.2.6 投票 (Voting)

**组合多个不同的模型**：

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

clf1 = LogisticRegression()
clf2 = DecisionTreeClassifier()
clf3 = SVC(probability=True)

voting_clf = VotingClassifier(
    estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)],
    voting='soft'  # 'hard' 或 'soft'
)

voting_clf.fit(X_train, y_train)
```

**投票方式**:
- `hard`: 多数投票
- `soft`: 平均预测概率

#### 4.2.7 堆叠 (Stacking)

**使用元学习器组合基学习器**：

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

estimators = [
    ('lr', LogisticRegression()),
    ('dt', DecisionTreeClassifier()),
    ('svc', SVC())
]

stacking_clf = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression()  # 元学习器
)

stacking_clf.fit(X_train, y_train)
```

#### 4.2.8 孤立森林 (IsolationForest)

**异常检测算法**：

```python
from sklearn.ensemble import IsolationForest

clf = IsolationForest(
    n_estimators=100,
    contamination=0.1,       # 异常值比例
    random_state=42
)

clf.fit(X_train)
y_pred = clf.predict(X_test)  # 1 为正常，-1 为异常
```

## 5. svm - 支持向量机

位置：`sklearn/svm/`

### 5.1 核心算法

#### 5.1.1 SVC (支持向量分类)

```python
from sklearn.svm import SVC

clf = SVC(
    C=1.0,                   # 正则化参数
    kernel='rbf',            # 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'
    degree=3,                # 多项式核的度数
    gamma='scale',           # 'scale', 'auto', 或浮点数
    probability=False        # 是否启用概率估计
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# 支持向量
print(clf.support_vectors_)
print(clf.n_support_)
```

**核函数**:
- `linear`: $K(x, x') = x^T x'$
- `poly`: $K(x, x') = (\gamma x^T x' + r)^d$
- `rbf`: $K(x, x') = \exp(-\gamma ||x - x'||^2)$
- `sigmoid`: $K(x, x') = \tanh(\gamma x^T x' + r)$

#### 5.1.2 LinearSVC

**线性 SVM（使用 liblinear）**：

```python
from sklearn.svm import LinearSVC

clf = LinearSVC(
    C=1.0,
    loss='squared_hinge',    # 'hinge', 'squared_hinge'
    penalty='l2',            # 'l1', 'l2'
    dual='auto',             # True 或 False
    max_iter=1000
)

clf.fit(X_train, y_train)
```

**优点**:
- 比 SVC(kernel='linear') 快
- 适合大规模数据

#### 5.1.3 SVR (支持向量回归)

```python
from sklearn.svm import SVR

reg = SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1              # epsilon-tube
)

reg.fit(X_train, y_train)
```

## 6. 其他监督学习模块

### 6.1 neural_network - 神经网络

```python
from sklearn.neural_network import MLPClassifier, MLPRegressor

clf = MLPClassifier(
    hidden_layer_sizes=(100,),  # 隐藏层结构
    activation='relu',           # 'relu', 'tanh', 'logistic'
    solver='adam',               # 'adam', 'sgd', 'lbfgs'
    alpha=0.0001,                # L2 正则化
    learning_rate='constant',    # 学习率策略
    max_iter=200
)

clf.fit(X_train, y_train)
```

### 6.2 neighbors - 最近邻

```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

clf = KNeighborsClassifier(
    n_neighbors=5,
    weights='uniform',           # 'uniform', 'distance'
    algorithm='auto',            # 'auto', 'ball_tree', 'kd_tree', 'brute'
    metric='minkowski'
)

clf.fit(X_train, y_train)
```

### 6.3 naive_bayes - 朴素贝叶斯

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# 高斯朴素贝叶斯
clf = GaussianNB()

# 多项式朴素贝叶斯（文本分类）
clf = MultinomialNB(alpha=1.0)

# 伯努利朴素贝叶斯（二值特征）
clf = BernoulliNB()

clf.fit(X_train, y_train)
```

### 6.4 discriminant_analysis - 判别分析

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

# 线性判别分析
lda = LinearDiscriminantAnalysis()

# 二次判别分析
qda = QuadraticDiscriminantAnalysis()

lda.fit(X_train, y_train)
```

### 6.5 gaussian_process - 高斯过程

```python
from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

kernel = 1.0 * RBF(1.0)
gpc = GaussianProcessClassifier(kernel=kernel)
gpc.fit(X_train, y_train)
```

## 7. 选择合适的算法

### 7.1 决策流程图

```
数据量大？
├─ 是 → 使用 SGD, LinearSVC, 或 HistGradientBoosting
└─ 否 → 继续

特征数 > 样本数？
├─ 是 → 使用线性模型（Ridge, Lasso, LinearSVC）
└─ 否 → 继续

需要概率估计？
├─ 是 → LogisticRegression, RandomForest, GradientBoosting
└─ 否 → 继续

分类还是回归？
├─ 分类 → RandomForest, GradientBoosting, SVC
└─ 回归 → RandomForest, GradientBoosting, SVR

需要可解释性？
├─ 是 → 线性模型, DecisionTree
└─ 否 → 集成方法

追求最高精度？
└─ GradientBoosting, HistGradientBoosting, StackingClassifier
```

### 7.2 算法对比

| 算法 | 训练速度 | 预测速度 | 准确率 | 可解释性 | 参数调优难度 |
|-----|---------|---------|-------|---------|------------|
| LogisticRegression | 快 | 快 | 中 | 高 | 低 |
| LinearSVC | 快 | 快 | 中 | 中 | 低 |
| DecisionTree | 中 | 快 | 中 | 高 | 中 |
| RandomForest | 中 | 中 | 高 | 低 | 中 |
| GradientBoosting | 慢 | 中 | 高 | 低 | 高 |
| HistGradientBoosting | 中 | 快 | 高 | 低 | 中 |
| SVC (RBF) | 慢 | 慢 | 高 | 低 | 高 |
| KNeighbors | 快 | 慢 | 中 | 中 | 低 |

## 8. 总结

scikit-learn 的监督学习模块提供了丰富的算法选择：

1. **线性模型**: 快速、可解释，适合大规模数据
2. **决策树**: 可解释性强，但易过拟合
3. **集成方法**: 准确率高，是竞赛和生产的首选
4. **SVM**: 强大但慢，适合中小数据集
5. **神经网络**: 灵活但需要大量数据

根据数据特点、性能要求和可解释性需求选择合适的算法。

---

**相关文档**:
- [01_scikit-learn架构总览.md](01_scikit-learn架构总览.md)
- [02_核心基类与API设计.md](02_核心基类与API设计.md)
- [04_无监督学习模块.md](04_无监督学习模块.md)
- [05_模型选择与评估.md](05_模型选择与评估.md)
