# scikit-learn 数据预处理与特征工程详解

## 1. 简介

数据预处理和特征工程是机器学习流程中最重要的步骤之一。scikit-learn 提供了丰富的工具来处理原始数据。

### 1.1 核心模块

| 模块 | 功能 |
|-----|------|
| `preprocessing` | 数据预处理（缩放、编码、变换） |
| `feature_extraction` | 特征提取（文本、图像） |
| `feature_selection` | 特征选择 |
| `impute` | 缺失值处理 |
| `pipeline` | 管道工具 |
| `compose` | 转换器组合 |

## 2. preprocessing - 数据预处理

位置：`sklearn/preprocessing/`

### 2.1 特征缩放

#### 2.1.1 StandardScaler - 标准化

**均值为 0，方差为 1**：

$$z = \frac{x - \mu}{\sigma}$$

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# 训练集拟合并转换
X_train_scaled = scaler.fit_transform(X_train)

# 测试集只转换（使用训练集的参数）
X_test_scaled = scaler.transform(X_test)

# 查看参数
print(scaler.mean_)       # 均值
print(scaler.scale_)      # 标准差
```

**特点**:
- 对异常值敏感
- 适合正态分布数据
- 大多数算法的默认选择

#### 2.1.2 MinMaxScaler - 归一化

**缩放到指定范围（默认 [0, 1]）**：

$$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$$

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# 缩放到其他范围
scaler = MinMaxScaler(feature_range=(-1, 1))
```

**特点**:
- 保持数据分布形状
- 对异常值敏感
- 适合神经网络

#### 2.1.3 RobustScaler - 鲁棒缩放

**使用中位数和四分位数，对异常值鲁棒**：

$$x' = \frac{x - Q_2}{Q_3 - Q_1}$$

```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
```

**特点**:
- 对异常值不敏感
- 适合有异常值的数据

#### 2.1.4 MaxAbsScaler

**除以最大绝对值**：

```python
from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X)  # 范围 [-1, 1]
```

**特点**:
- 不破坏稀疏性
- 适合稀疏数据

#### 2.1.5 Normalizer - 归一化每个样本

**将每个样本缩放到单位范数**：

```python
from sklearn.preprocessing import Normalizer

normalizer = Normalizer(norm='l2')  # 'l1', 'l2', 'max'
X_normalized = normalizer.fit_transform(X)

# 每个样本的 L2 范数为 1
print(np.linalg.norm(X_normalized[0]))  # 1.0
```

**应用场景**:
- 文本分类
- 余弦相似度计算

### 2.2 分类特征编码

#### 2.2.1 LabelEncoder - 标签编码

**将类别转换为整数**：

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_encoded = le.fit_transform(['red', 'blue', 'green', 'red'])
# array([2, 0, 1, 2])

# 逆转换
y_original = le.inverse_transform([2, 0, 1, 2])
# ['red', 'blue', 'green', 'red']

# 查看映射
print(le.classes_)  # ['blue', 'green', 'red']
```

**注意**:
- 仅用于目标变量
- 不要用于特征（会引入顺序关系）

#### 2.2.2 OneHotEncoder - 独热编码

**创建虚拟变量**：

```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

X = [['red'], ['blue'], ['green'], ['red']]
X_encoded = encoder.fit_transform(X)
# array([[0, 0, 1],    # red
#        [1, 0, 0],    # blue
#        [0, 1, 0],    # green
#        [0, 0, 1]])   # red

print(encoder.categories_)  # [array(['blue', 'green', 'red'])]

# 逆转换
X_original = encoder.inverse_transform(X_encoded)
```

**参数**:
- `sparse_output`: 是否返回稀疏矩阵
- `handle_unknown='ignore'`: 遇到未知类别时处理方式
- `drop='first'`: 删除第一列（避免多重共线性）

#### 2.2.3 OrdinalEncoder - 序数编码

**为多个特征编码**：

```python
from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder()

X = [['red', 'S'], 
     ['blue', 'M'], 
     ['green', 'L']]
     
X_encoded = encoder.fit_transform(X)
# array([[2., 2.],
#        [0., 1.],
#        [1., 0.]])
```

**与 LabelEncoder 的区别**:
- OrdinalEncoder: 用于特征矩阵（2D）
- LabelEncoder: 用于单个数组（1D）

### 2.3 非线性变换

#### 2.3.1 QuantileTransformer - 分位数转换

**将特征转换为均匀或正态分布**：

```python
from sklearn.preprocessing import QuantileTransformer

# 转换为均匀分布
qt_uniform = QuantileTransformer(output_distribution='uniform', n_quantiles=1000)
X_uniform = qt_uniform.fit_transform(X)

# 转换为正态分布
qt_normal = QuantileTransformer(output_distribution='normal')
X_normal = qt_normal.fit_transform(X)
```

**特点**:
- 对异常值鲁棒
- 平滑单调变换
- 可能破坏特征间的相关性

#### 2.3.2 PowerTransformer - 幂变换

**使数据更接近正态分布**：

```python
from sklearn.preprocessing import PowerTransformer

# Yeo-Johnson 变换（支持负值）
pt_yj = PowerTransformer(method='yeo-johnson')
X_yj = pt_yj.fit_transform(X)

# Box-Cox 变换（仅正值）
pt_bc = PowerTransformer(method='box-cox')
X_bc = pt_bc.fit_transform(X_positive)
```

#### 2.3.3 PolynomialFeatures - 多项式特征

**生成多项式和交互特征**：

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)

X = [[1, 2], [3, 4]]
X_poly = poly.fit_transform(X)
# array([[1, 2, 1, 2, 4],       # [x1, x2, x1^2, x1*x2, x2^2]
#        [3, 4, 9, 12, 16]])

print(poly.get_feature_names_out(['x1', 'x2']))
# ['x1', 'x2', 'x1^2', 'x1 x2', 'x2^2']
```

**参数**:
- `degree`: 多项式的度数
- `interaction_only=True`: 仅交互项
- `include_bias=False`: 不包括常数项

#### 2.3.4 FunctionTransformer - 自定义变换

```python
from sklearn.preprocessing import FunctionTransformer
import numpy as np

# 对数变换
log_transformer = FunctionTransformer(np.log1p, inverse_func=np.expm1)
X_log = log_transformer.fit_transform(X)

# 自定义函数
def custom_transform(X):
    return X ** 2

custom_transformer = FunctionTransformer(custom_transform)
```

### 2.4 离散化

#### 2.4.1 KBinsDiscretizer - 分箱

**将连续特征离散化**：

```python
from sklearn.preprocessing import KBinsDiscretizer

discretizer = KBinsDiscretizer(
    n_bins=5,
    encode='ordinal',    # 'ordinal', 'onehot', 'onehot-dense'
    strategy='quantile'  # 'uniform', 'quantile', 'kmeans'
)

X_binned = discretizer.fit_transform(X)

# 查看箱边界
print(discretizer.bin_edges_)
```

**策略**:
- `uniform`: 等宽分箱
- `quantile`: 等频分箱
- `kmeans`: K-means 聚类

#### 2.4.2 Binarizer - 二值化

```python
from sklearn.preprocessing import Binarizer

binarizer = Binarizer(threshold=0.5)
X_binary = binarizer.fit_transform(X)
# 大于 0.5 -> 1, 否则 -> 0
```

### 2.5 其他转换

#### 2.5.1 LabelBinarizer

**将标签二值化为多列**：

```python
from sklearn.preprocessing import LabelBinarizer

lb = LabelBinarizer()
y = ['red', 'blue', 'green', 'red']
y_binary = lb.fit_transform(y)
# array([[0, 0, 1],
#        [1, 0, 0],
#        [0, 1, 0],
#        [0, 0, 1]])
```

#### 2.5.2 MultiLabelBinarizer

**多标签二值化**：

```python
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
y = [['red', 'blue'], ['green'], ['red', 'green']]
y_binary = mlb.fit_transform(y)
```

## 3. feature_extraction - 特征提取

位置：`sklearn/feature_extraction/`

### 3.1 文本特征提取

#### 3.1.1 CountVectorizer - 词袋模型

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
]

vectorizer = CountVectorizer(
    max_features=1000,       # 最大特征数
    min_df=1,                # 最小文档频率
    max_df=0.8,              # 最大文档频率
    stop_words='english',    # 停用词
    ngram_range=(1, 2)       # n-gram 范围
)

X = vectorizer.fit_transform(corpus)
print(X.toarray())

# 特征名称
print(vectorizer.get_feature_names_out())

# 词汇表
print(vectorizer.vocabulary_)
```

#### 3.1.2 TfidfVectorizer - TF-IDF

**词频-逆文档频率**：

$$tfidf(t,d) = tf(t,d) \times idf(t)$$

$$idf(t) = \log\frac{n}{df(t)} + 1$$

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=1000,
    min_df=2,
    max_df=0.8,
    stop_words='english',
    ngram_range=(1, 2),
    use_idf=True,
    smooth_idf=True,
    sublinear_tf=False
)

X = vectorizer.fit_transform(corpus)
```

#### 3.1.3 HashingVectorizer

**使用哈希技巧**，节省内存：

```python
from sklearn.feature_extraction.text import HashingVectorizer

vectorizer = HashingVectorizer(n_features=2**10)
X = vectorizer.fit_transform(corpus)
```

**特点**:
- 无需存储词汇表
- 适合大规模数据
- 不可逆（无法获取特征名）

### 3.2 图像特征提取

```python
from sklearn.feature_extraction import image

# 从图像提取 patch
patches = image.extract_patches_2d(image, patch_size=(7, 7))

# 重构图像
reconstructed = image.reconstruct_from_patches_2d(patches, image.shape)
```

## 4. feature_selection - 特征选择

位置：`sklearn/feature_selection/`

### 4.1 过滤方法

#### 4.1.1 VarianceThreshold - 方差阈值

**移除低方差特征**：

```python
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0.1)
X_selected = selector.fit_transform(X)

# 被选择的特征
print(selector.get_support())
```

#### 4.1.2 SelectKBest - 选择 K 个最佳特征

```python
from sklearn.feature_selection import SelectKBest, chi2, f_classif

# 卡方检验（分类，非负特征）
selector = SelectKBest(score_func=chi2, k=10)
X_selected = selector.fit_transform(X, y)

# ANOVA F 值（分类）
selector = SelectKBest(score_func=f_classif, k=10)

# 查看分数
scores = selector.scores_
print(f"Feature scores: {scores}")

# 被选择的特征
print(selector.get_support())
```

**评分函数**:
- `chi2`: 卡方检验（分类，非负）
- `f_classif`: ANOVA F 值（分类）
- `f_regression`: F 值（回归）
- `mutual_info_classif`: 互信息（分类）
- `mutual_info_regression`: 互信息（回归）

#### 4.1.3 SelectPercentile

**选择百分比的最佳特征**：

```python
from sklearn.feature_selection import SelectPercentile

selector = SelectPercentile(score_func=f_classif, percentile=10)
X_selected = selector.fit_transform(X, y)
```

### 4.2 包装方法

#### 4.2.1 RFE - 递归特征消除

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

estimator = RandomForestClassifier(n_estimators=100)
selector = RFE(
    estimator=estimator,
    n_features_to_select=10,  # 要选择的特征数
    step=1                     # 每次移除的特征数
)

selector.fit(X, y)
X_selected = selector.transform(X)

# 特征排名（1 = 被选择）
print(selector.ranking_)

# 被选择的特征
print(selector.support_)
```

#### 4.2.2 RFECV - 带交叉验证的 RFE

**自动确定最佳特征数**：

```python
from sklearn.feature_selection import RFECV

selector = RFECV(
    estimator=RandomForestClassifier(n_estimators=100),
    step=1,
    cv=5,
    scoring='accuracy'
)

selector.fit(X, y)
X_selected = selector.transform(X)

print(f"Optimal number of features: {selector.n_features_}")

# 绘制特征数 vs 分数
import matplotlib.pyplot as plt
plt.plot(range(1, len(selector.cv_results_['mean_test_score']) + 1),
         selector.cv_results_['mean_test_score'])
plt.xlabel('Number of features')
plt.ylabel('CV Score')
plt.show()
```

### 4.3 嵌入方法

#### 4.3.1 SelectFromModel - 基于模型的选择

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

# 基于特征重要性
estimator = RandomForestClassifier(n_estimators=100)
estimator.fit(X, y)

selector = SelectFromModel(estimator, threshold='median', prefit=True)
X_selected = selector.transform(X)

# 或者一步完成
selector = SelectFromModel(estimator, threshold='median')
X_selected = selector.fit_transform(X, y)
```

**支持的模型**:
- 树模型（feature_importances_）
- 线性模型（coef_）

#### 4.3.2 L1 正则化特征选择

```python
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso = Lasso(alpha=0.1)
selector = SelectFromModel(lasso)
X_selected = selector.fit_transform(X, y)

# 查看被选择的特征
print(selector.get_support())
```

### 4.4 SequentialFeatureSelector - 序列特征选择

```python
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)

# 前向选择
sfs_forward = SequentialFeatureSelector(
    knn, 
    n_features_to_select=5,
    direction='forward',
    cv=5
)
sfs_forward.fit(X, y)

# 后向选择
sfs_backward = SequentialFeatureSelector(
    knn, 
    n_features_to_select=5,
    direction='backward',
    cv=5
)
sfs_backward.fit(X, y)
```

## 5. impute - 缺失值处理

位置：`sklearn/impute/`

### 5.1 SimpleImputer - 简单填充

```python
from sklearn.impute import SimpleImputer

# 均值填充
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# 其他策略
imputer_median = SimpleImputer(strategy='median')
imputer_most_frequent = SimpleImputer(strategy='most_frequent')
imputer_constant = SimpleImputer(strategy='constant', fill_value=0)
```

### 5.2 IterativeImputer - 迭代填充

**使用其他特征预测缺失值**：

```python
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(
    estimator=None,          # 默认使用 BayesianRidge
    max_iter=10,
    random_state=42
)

X_imputed = imputer.fit_transform(X)
```

### 5.3 KNNImputer - K 近邻填充

```python
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5, weights='uniform')
X_imputed = imputer.fit_transform(X)
```

## 6. Pipeline 和 ColumnTransformer

### 6.1 Pipeline - 管道

**串联多个转换器和估计器**：

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('classifier', LogisticRegression())
])

# 一步完成所有操作
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

# 访问中间步骤
print(pipeline.named_steps['pca'].explained_variance_ratio_)

# 在 GridSearch 中使用
param_grid = {
    'pca__n_components': [10, 20, 30],
    'classifier__C': [0.1, 1, 10]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

### 6.2 ColumnTransformer - 列转换器

**对不同列应用不同的转换**：

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# 假设有数值和类别特征
numeric_features = ['age', 'salary']
categorical_features = ['gender', 'department']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'  # 其他列保持不变
)

# 组合成完整管道
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression())
])

pipeline.fit(X_train, y_train)
```

### 6.3 make_column_transformer - 简化语法

```python
from sklearn.compose import make_column_transformer

preprocessor = make_column_transformer(
    (StandardScaler(), numeric_features),
    (OneHotEncoder(), categorical_features),
    remainder='passthrough'
)
```

### 6.4 make_pipeline - 简化 Pipeline

```python
from sklearn.pipeline import make_pipeline

# 自动命名
pipeline = make_pipeline(
    StandardScaler(),
    PCA(n_components=0.95),
    LogisticRegression()
)
```

## 7. 实用示例

### 7.1 完整的数据预处理流程

```python
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# 数据
df = pd.DataFrame({
    'age': [25, 30, None, 40],
    'salary': [50000, 60000, 70000, None],
    'gender': ['M', 'F', 'M', 'F'],
    'city': ['NY', 'LA', None, 'NY']
})

# 区分数值和类别特征
numeric_features = ['age', 'salary']
categorical_features = ['gender', 'city']

# 数值特征管道
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 类别特征管道
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 组合
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# 完整管道
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# 训练
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```

### 7.2 文本分类管道

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import LinearSVC

text_clf = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),
    ('feature_selection', SelectKBest(chi2, k=5000)),
    ('classifier', LinearSVC())
])

text_clf.fit(X_train, y_train)
```

## 8. 特征工程最佳实践

### 8.1 特征缩放的选择

| 场景 | 推荐方法 |
|-----|---------|
| 正态分布数据 | StandardScaler |
| 有异常值 | RobustScaler |
| 需要 [0,1] 范围 | MinMaxScaler |
| 稀疏数据 | MaxAbsScaler |
| 树模型 | 不需要缩放 |

### 8.2 编码方式的选择

| 特征类型 | 推荐方法 |
|---------|---------|
| 无序类别 | OneHotEncoder |
| 有序类别 | OrdinalEncoder |
| 高基数类别 | TargetEncoder, HashingEncoder |
| 目标变量 | LabelEncoder |

### 8.3 避免数据泄漏

```python
# 错误：在划分前进行特征选择
selector = SelectKBest(k=10)
X_selected = selector.fit_transform(X, y)  # 使用了所有数据
X_train, X_test = train_test_split(X_selected, y)

# 正确：使用 Pipeline
pipeline = Pipeline([
    ('selector', SelectKBest(k=10)),
    ('classifier', LogisticRegression())
])

X_train, X_test, y_train, y_test = train_test_split(X, y)
pipeline.fit(X_train, y_train)  # selector 只在训练集上拟合
```

## 9. 总结

数据预处理和特征工程的关键要点：

1. **特征缩放**: 根据数据分布选择合适的方法
2. **编码**: 正确处理类别特征
3. **缺失值**: 选择合适的填充策略
4. **特征选择**: 减少维度，提高性能
5. **Pipeline**: 避免数据泄漏，简化工作流程
6. **ColumnTransformer**: 对不同类型的特征应用不同的转换

---

**相关文档**:
- [01_scikit-learn架构总览.md](01_scikit-learn架构总览.md)
- [02_核心基类与API设计.md](02_核心基类与API设计.md)
- [03_监督学习模块.md](03_监督学习模块.md)
- [05_模型选择与评估.md](05_模型选择与评估.md)
