# MNE-Python scikit-learn ä¾èµ–è¯¦ç»†åˆ†æ

> **å¯é€‰ä¾èµ–**: `scikit-learn >= 1.3`  
> **ä½¿ç”¨é¢‘ç‡**: ğŸ”¥ğŸ”¥ğŸ”¥ (è§£ç åˆ†ææ¨¡å—)  
> **è§’è‰²**: æœºå™¨å­¦ä¹ ã€åˆ†ç±»ã€å›å½’ã€é™ç»´ã€äº¤å‰éªŒè¯

---

## ç›®å½•

1. [scikit-learn åœ¨ MNE ä¸­çš„å®šä½](#scikit-learn-åœ¨-mne-ä¸­çš„å®šä½)
2. [mne.decoding æ¨¡å—æ¶æ„](#mnedecoding-æ¨¡å—æ¶æ„)
3. [åˆ†ç±»å™¨é›†æˆ](#åˆ†ç±»å™¨é›†æˆ)
4. [ç‰¹å¾æå–ä¸é™ç»´](#ç‰¹å¾æå–ä¸é™ç»´)
5. [äº¤å‰éªŒè¯ç­–ç•¥](#äº¤å‰éªŒè¯ç­–ç•¥)
6. [Pipeline è®¾è®¡æ¨¡å¼](#pipeline-è®¾è®¡æ¨¡å¼)
7. [Transformer API å®ç°](#transformer-api-å®ç°)
8. [å®Œæ•´å·¥ä½œæµç¤ºä¾‹](#å®Œæ•´å·¥ä½œæµç¤ºä¾‹)

---

## scikit-learn åœ¨ MNE ä¸­çš„å®šä½

### 1. å¯é€‰ä½†æ ¸å¿ƒ

**ä¾èµ–å£°æ˜**: `scikit-learn >= 1.3` (åœ¨ `mne[full]` ä¸­)

**ä½¿ç”¨åœºæ™¯**:
- âœ… **è§£ç åˆ†æ**: `mne.decoding` æ¨¡å—
- âœ… **æœºå™¨å­¦ä¹ **: åˆ†ç±»ã€å›å½’ã€èšç±»
- âœ… **ç‰¹å¾å·¥ç¨‹**: PCA, ICA, CSP, SSD
- âœ… **æ¨¡å‹è¯„ä¼°**: äº¤å‰éªŒè¯ã€è¯„åˆ†
- âŒ **å¿…éœ€åŠŸèƒ½**: ä¸å½±å“ I/Oã€é¢„å¤„ç†ã€å¯è§†åŒ–

**å®‰è£…æ£€æŸ¥**:
```python
import mne

# å¦‚æœæœªå®‰è£… sklearn
try:
    from sklearn import __version__
except ImportError:
    print("sklearn not installed - decoding module unavailable")
```

---

### 2. sklearn æ¨¡å—ä½¿ç”¨ç»Ÿè®¡

| sklearn æ¨¡å— | ä½¿ç”¨ä½ç½® | ä¸»è¦ç±»/å‡½æ•° | MNE åº”ç”¨ |
|-------------|---------|------------|---------|
| **sklearn.base** | `mne/decoding/base.py` | `BaseEstimator`, `TransformerMixin` | è‡ªå®šä¹‰ Transformer |
| **sklearn.model_selection** | `mne/decoding/` | `KFold`, `cross_val_score` | äº¤å‰éªŒè¯ |
| **sklearn.linear_model** | `mne/decoding/`, `mne/preprocessing/` | `LogisticRegression`, `Ridge` | åˆ†ç±»ã€å›å½’ |
| **sklearn.decomposition** | `mne/preprocessing/ica.py` | `FastICA`, `PCA` | ICA, é™ç»´ |
| **sklearn.discriminant_analysis** | `mne/decoding/` | `LinearDiscriminantAnalysis` | LDA åˆ†ç±»å™¨ |
| **sklearn.svm** | `mne/decoding/` | `SVC`, `SVR` | æ”¯æŒå‘é‡æœº |
| **sklearn.preprocessing** | `mne/decoding/transformer.py` | `StandardScaler`, `RobustScaler` | æ ‡å‡†åŒ– |
| **sklearn.pipeline** | `mne/decoding/` | `Pipeline`, `make_pipeline` | å·¥ä½œæµç»„åˆ |
| **sklearn.metrics** | `mne/decoding/` | `accuracy_score`, `r2_score` | æ€§èƒ½è¯„ä¼° |
| **sklearn.feature_extraction** | `mne/stats/` | `grid_to_graph` | ç©ºé—´é‚»æ¥ |
| **sklearn.neighbors** | `mne/surface.py` | `BallTree`, `LocalOutlierFactor` | è¿‘é‚»ã€å¼‚å¸¸æ£€æµ‹ |

---

## mne.decoding æ¨¡å—æ¶æ„

### 1. æ¨¡å—ç»“æ„

```
mne/decoding/
â”œâ”€â”€ __init__.py              # å…¬å¼€ API
â”œâ”€â”€ base.py                  # åŸºç¡€ç±» (BaseEstimator å¤åˆ¶)
â”œâ”€â”€ csp.py                   # Common Spatial Pattern
â”œâ”€â”€ ems.py                   # Event-Matched Spatial filter
â”œâ”€â”€ _ged.py                  # Generalized Eigenvalue Decomposition
â”œâ”€â”€ receptive_field.py       # Receptive Field æ¨¡å‹
â”œâ”€â”€ search_light.py          # Searchlight åˆ†æ
â”œâ”€â”€ ssd.py                   # Spatio-Spectral Decomposition
â”œâ”€â”€ time_delaying_ridge.py   # Time-Delaying Ridge Regression
â”œâ”€â”€ time_frequency.py        # æ—¶é¢‘ç‰¹å¾
â”œâ”€â”€ transformer.py           # Scaler, Vectorizer, FilterEstimator
â”œâ”€â”€ xdawn.py                 # Xdawn (ä¹Ÿåœ¨ preprocessing)
â””â”€â”€ tests/                   # å•å…ƒæµ‹è¯•
```

---

### 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

**éµå¾ª sklearn API æ ‡å‡†**:

```python
# sklearn æ ‡å‡†æ¥å£
class MyTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        """å­¦ä¹ å‚æ•°"""
        # ... è®¡ç®—ç»Ÿè®¡é‡
        return self
    
    def transform(self, X):
        """åº”ç”¨å˜æ¢"""
        # ... è½¬æ¢æ•°æ®
        return X_transformed
    
    # å¯é€‰: fit_transform (è‡ªåŠ¨å®ç°)
    # def fit_transform(self, X, y=None):
    #     return self.fit(X, y).transform(X)
```

**MNE æ‰©å±•**:
```python
from mne.decoding import Scaler

# æ·»åŠ  MNE ç‰¹å®šåŠŸèƒ½
class Scaler(TransformerMixin):
    def __init__(self, info=None, scalings='mean'):
        self.info = info  # MNE Info å¯¹è±¡
        self.scalings = scalings
    
    def fit(self, X, y=None):
        # X: (n_epochs, n_channels, n_times)
        self.mean_ = X.mean(axis=(0, 2), keepdims=True)
        self.std_ = X.std(axis=(0, 2), keepdims=True)
        return self
    
    def transform(self, X):
        return (X - self.mean_) / self.std_
```

---

## åˆ†ç±»å™¨é›†æˆ

### 1. å¸¸ç”¨åˆ†ç±»å™¨

**ä½ç½®**: `mne/decoding/base.py`, `mne/decoding/tests/`

```python
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC

# 1. Logistic Regression (é»˜è®¤)
clf_lr = LogisticRegression(
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)

# 2. LDA (é™ç»´ + åˆ†ç±»)
clf_lda = LinearDiscriminantAnalysis(
    solver='lsqr',  # 'svd', 'lsqr', 'eigen'
    shrinkage='auto'
)

# 3. SVM
clf_svm = SVC(
    kernel='rbf',   # 'linear', 'poly', 'rbf'
    C=1.0,
    gamma='scale'
)
```

---

### 2. åˆ†ç±»æµç¨‹ç¤ºä¾‹

**ä½ç½®**: `examples/decoding/decoding_csp_timefreq.py`

```python
from mne.decoding import CSP
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# å‡†å¤‡æ•°æ®
epochs = mne.Epochs(raw, events, tmin=0, tmax=1, baseline=None)
X = epochs.get_data()  # (n_epochs, n_channels, n_times)
y = epochs.events[:, -1]  # ç±»åˆ«æ ‡ç­¾

# æ„å»º Pipeline
csp = CSP(n_components=4, reg=None, log=True)
clf = LogisticRegression(max_iter=1000)
pipeline = make_pipeline(csp, clf)

# äº¤å‰éªŒè¯
scores = cross_val_score(
    pipeline, X, y, 
    cv=5,                          # 5-fold
    scoring='accuracy',
    n_jobs=-1                      # å¹¶è¡Œ
)

print(f"Accuracy: {scores.mean():.2f} Â± {scores.std():.2f}")
```

---

### 3. åˆ†ç±»å™¨åŒ…è£… (SlidingEstimator)

**ä½ç½®**: `mne/decoding/base.py`

```python
from mne.decoding import SlidingEstimator
from sklearn.linear_model import LogisticRegression

# åœ¨æ¯ä¸ªæ—¶é—´ç‚¹è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨
sliding = SlidingEstimator(
    LogisticRegression(),
    n_jobs=4,
    scoring='roc_auc'
)

# æ‹Ÿåˆ
sliding.fit(X, y)  # X: (n_epochs, n_channels, n_times)

# é¢„æµ‹
y_pred = sliding.predict(X_test)  # (n_epochs, n_times)

# å¾—åˆ† (æ—¶é—´åºåˆ—)
scores = sliding.score(X_test, y_test)  # (n_times,)
```

**åº”ç”¨**: è§£ç æ—¶é—´åŠ¨æ€ (temporal decoding)

---

## ç‰¹å¾æå–ä¸é™ç»´

### 1. PCA - ä¸»æˆåˆ†åˆ†æ

**ä½ç½®**: `mne/preprocessing/tests/test_infomax.py`

```python
from sklearn.decomposition import PCA

# é™ç»´
pca = PCA(n_components=0.95,  # ä¿ç•™ 95% æ–¹å·®
          whiten=True)

X_pca = pca.fit_transform(X)

# è§£é‡Šæ–¹å·®
explained_var = pca.explained_variance_ratio_
print(f"Components: {pca.n_components_}")
print(f"Explained variance: {explained_var.sum():.2%}")
```

---

### 2. ICA - ç‹¬ç«‹æˆåˆ†åˆ†æ

**ä½ç½®**: `mne/preprocessing/ica.py`

```python
from sklearn.decomposition import FastICA

class ICA:
    def __init__(self, method='fastica', ...):
        if method == 'fastica':
            from sklearn.decomposition import FastICA
            
            self._ica = FastICA(
                n_components=n_components,
                algorithm='parallel',  # 'parallel', 'deflation'
                fun='logcosh',         # 'logcosh', 'exp', 'cube'
                max_iter=200,
                random_state=random_state
            )
    
    def fit(self, inst):
        data = inst.get_data()  # (n_channels, n_times)
        
        # sklearn FastICA
        self._ica.fit(data.T)  # è½¬ç½®: (n_times, n_channels)
        
        # æå–æ··åˆçŸ©é˜µå’Œè§£æ··çŸ©é˜µ
        self.mixing_matrix_ = self._ica.mixing_  # (n_channels, n_components)
        self.unmixing_matrix_ = self._ica.components_  # (n_components, n_channels)
        
        return self
```

---

### 3. CSP - å…±ç©ºé—´æ¨¡å¼

**ä½ç½®**: `mne/decoding/csp.py`

```python
from mne.decoding import CSP

# CSP åŸºäºå¹¿ä¹‰ç‰¹å¾å€¼åˆ†è§£
csp = CSP(
    n_components=4,      # æå– 4 ä¸ªç©ºé—´æ»¤æ³¢å™¨
    reg=None,            # æ­£åˆ™åŒ– (None, 'shrinkage', float)
    log=True,            # å¯¹ç‰¹å¾å–å¯¹æ•°
    cov_est='concat',    # åæ–¹å·®ä¼°è®¡ ('concat', 'epoch')
    transform_into='average_power'  # 'average_power', 'csp_space'
)

# æ‹Ÿåˆ (éœ€è¦ä¸¤ç±»æ•°æ®)
csp.fit(X, y)  # X: (n_epochs, n_channels, n_times), y: ç±»åˆ«æ ‡ç­¾

# å˜æ¢
X_csp = csp.transform(X)  # (n_epochs, n_components * 2)
# å‰ n_components ä¸ª: ç±» 1 æœ€å¤§æ–¹å·®
# å n_components ä¸ª: ç±» 2 æœ€å¤§æ–¹å·®

# ç©ºé—´æ¨¡å¼
patterns = csp.patterns_  # (n_channels, n_components * 2)
```

**CSP å·¥ä½œåŸç†**:
1. è®¡ç®—ä¸¤ç±»åæ–¹å·®çŸ©é˜µ: C1, C2
2. æ±‚è§£å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜: C1 v = Î» (C1 + C2) v
3. é€‰æ‹©æœ€å¤§å’Œæœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
4. æŠ•å½±æ•°æ®åˆ° CSP ç©ºé—´

---

### 4. SSD - æ—¶ç©ºè°±åˆ†è§£

**ä½ç½®**: `mne/decoding/ssd.py`

```python
from mne.decoding import SSD

# é’ˆå¯¹ç‰¹å®šé¢‘æ®µçš„ç©ºé—´æ»¤æ³¢
ssd = SSD(
    info=epochs.info,
    filt_params_signal=(8, 12),   # ä¿¡å·é¢‘æ®µ (alpha)
    filt_params_noise=(6, 7, 13, 14),  # å™ªå£°é¢‘æ®µ
    reg='oas',                     # åæ–¹å·®æ­£åˆ™åŒ–
    n_components=4
)

# æ‹Ÿåˆ
ssd.fit(X)

# æå–æˆåˆ†
X_ssd = ssd.transform(X)

# å¯è§†åŒ–ç©ºé—´æ¨¡å¼
ssd.plot_patterns(epochs.info)
```

---

## äº¤å‰éªŒè¯ç­–ç•¥

### 1. KFold äº¤å‰éªŒè¯

**ä½ç½®**: `mne/decoding/base.py`

```python
from sklearn.model_selection import KFold, StratifiedKFold

# K-Fold (å›å½’ä»»åŠ¡)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, test_idx in kf.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # è®­ç»ƒå’Œè¯„ä¼°

# Stratified K-Fold (åˆ†ç±»ä»»åŠ¡ - ä¿æŒç±»åˆ«æ¯”ä¾‹)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, test_idx in skf.split(X, y):
    # ç¡®ä¿æ¯æŠ˜ä¸­ç±»åˆ«åˆ†å¸ƒä¸€è‡´
    ...
```

---

### 2. cross_val_score

**ä½ç½®**: `mne/decoding/tests/`

```python
from sklearn.model_selection import cross_val_score

# æ–¹ä¾¿çš„ CV è¯„åˆ†
scores = cross_val_score(
    estimator=pipeline,
    X=X, y=y,
    cv=5,                    # æŠ˜æ•°æˆ– CV å¯¹è±¡
    scoring='accuracy',      # 'accuracy', 'roc_auc', 'r2', ...
    n_jobs=-1,               # å¹¶è¡Œ (æ‰€æœ‰ CPU)
    verbose=1
)

print(f"Scores: {scores}")
print(f"Mean: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

---

### 3. GeneralizingEstimator

**ä½ç½®**: `mne/decoding/base.py`

```python
from mne.decoding import GeneralizingEstimator

# åœ¨æ—¶é—´ç‚¹ i è®­ç»ƒï¼Œåœ¨æ—¶é—´ç‚¹ j æµ‹è¯•
gen = GeneralizingEstimator(
    LogisticRegression(),
    n_jobs=4,
    scoring='roc_auc'
)

gen.fit(X_train, y_train)

# æ³›åŒ–çŸ©é˜µ (train_time x test_time)
scores = gen.score(X_test, y_test)  # (n_times, n_times)

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.imshow(scores, origin='lower', cmap='RdBu_r')
plt.xlabel('Testing Time')
plt.ylabel('Training Time')
plt.colorbar(label='ROC AUC')
```

---

## Pipeline è®¾è®¡æ¨¡å¼

### 1. make_pipeline ç®€åŒ–

**ä½ç½®**: `examples/decoding/`

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from mne.decoding import Scaler, Vectorizer

# æ–¹æ³• 1: sklearn Pipeline
pipeline_sklearn = make_pipeline(
    StandardScaler(),              # æ ‡å‡†åŒ–
    LogisticRegression()           # åˆ†ç±»å™¨
)

# æ–¹æ³• 2: MNE + sklearn æ··åˆ
pipeline_mne = make_pipeline(
    Scaler(scalings='mean'),       # MNE Scaler (ä¿ç•™ 3D å½¢çŠ¶)
    Vectorizer(),                  # å±•å¹³ä¸º 2D
    StandardScaler(),              # sklearn æ ‡å‡†åŒ–
    LogisticRegression()
)

# è®­ç»ƒ
pipeline_mne.fit(X, y)  # X: (n_epochs, n_channels, n_times)

# é¢„æµ‹
y_pred = pipeline_mne.predict(X_test)
```

---

### 2. è‡ªå®šä¹‰ Pipeline æ­¥éª¤

```python
from sklearn.base import BaseEstimator, TransformerMixin

class EpochsVectorizer(BaseEstimator, TransformerMixin):
    """å±•å¹³ MNE Epochs æ•°æ®"""
    
    def fit(self, X, y=None):
        return self  # æ— å‚æ•°éœ€è¦å­¦ä¹ 
    
    def transform(self, X):
        # X: (n_epochs, n_channels, n_times)
        n_epochs = X.shape[0]
        return X.reshape(n_epochs, -1)  # (n_epochs, n_channels * n_times)

# ä½¿ç”¨
pipeline = make_pipeline(
    EpochsVectorizer(),
    StandardScaler(),
    LogisticRegression()
)
```

---

### 3. Pipeline å‚æ•°è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV

# å®šä¹‰ Pipeline
pipeline = make_pipeline(
    CSP(n_components=4),
    LogisticRegression()
)

# å‚æ•°ç½‘æ ¼ (ä½¿ç”¨ stepåç§°__å‚æ•°å)
param_grid = {
    'csp__n_components': [2, 4, 6, 8],
    'csp__reg': [None, 'ledoit_wolf', 0.1],
    'logisticregression__C': [0.01, 0.1, 1, 10],
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=5, 
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X, y)

print(f"Best params: {grid_search.best_params_}")
print(f"Best score: {grid_search.best_score_:.3f}")
```

---

## Transformer API å®ç°

### 1. Scaler - æ ‡å‡†åŒ–

**ä½ç½®**: `mne/decoding/transformer.py`

```python
from sklearn.preprocessing import StandardScaler, RobustScaler

class Scaler(TransformerMixin):
    """ä¿ç•™ 3D æ•°ç»„å½¢çŠ¶çš„ Scaler"""
    
    def __init__(self, info=None, scalings='mean', with_mean=True, with_std=True):
        self.scalings = scalings
        self.with_mean = with_mean
        self.with_std = with_std
        
        if scalings == 'mean':
            self._scaler = StandardScaler(
                with_mean=with_mean, 
                with_std=with_std
            )
        elif scalings == 'median':
            self._scaler = RobustScaler(
                with_centering=with_mean,
                with_scaling=with_std
            )
    
    def fit(self, X, y=None):
        # X: (n_epochs, n_channels, n_times)
        n_epochs, n_channels, n_times = X.shape
        
        # æ²¿ epochs å’Œ times è½´æ ‡å‡†åŒ–
        X_2d = X.transpose(1, 0, 2).reshape(n_channels, -1).T
        # X_2d: (n_epochs * n_times, n_channels)
        
        self._scaler.fit(X_2d)
        return self
    
    def transform(self, X):
        n_epochs, n_channels, n_times = X.shape
        X_2d = X.transpose(1, 0, 2).reshape(n_channels, -1).T
        
        X_scaled = self._scaler.transform(X_2d)
        
        # æ¢å¤ 3D å½¢çŠ¶
        X_scaled = X_scaled.T.reshape(n_channels, n_epochs, n_times)
        X_scaled = X_scaled.transpose(1, 0, 2)
        
        return X_scaled
```

---

### 2. Vectorizer - å±•å¹³

**ä½ç½®**: `mne/decoding/transformer.py`

```python
class Vectorizer(TransformerMixin):
    """å°† 3D epochs å±•å¹³ä¸º 2D"""
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        # X: (n_epochs, n_channels, n_times)
        n_epochs = X.shape[0]
        return X.reshape(n_epochs, -1)
    
    def inverse_transform(self, X):
        # æ¢å¤åŸå§‹å½¢çŠ¶ (éœ€è¦è®°å½•)
        n_epochs = X.shape[0]
        return X.reshape(n_epochs, self.n_channels_, self.n_times_)
```

---

### 3. FilterEstimator - æ»¤æ³¢åŒ…è£…

**ä½ç½®**: `mne/decoding/transformer.py`

```python
class FilterEstimator(TransformerMixin):
    """åœ¨ sklearn Pipeline ä¸­åº”ç”¨ MNE æ»¤æ³¢"""
    
    def __init__(self, info, l_freq, h_freq, method='fir'):
        self.info = info
        self.l_freq = l_freq
        self.h_freq = h_freq
        self.method = method
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        from mne.filter import filter_data
        
        # X: (n_epochs, n_channels, n_times)
        X_filt = np.empty_like(X)
        
        for i in range(X.shape[0]):
            X_filt[i] = filter_data(
                X[i], 
                sfreq=self.info['sfreq'],
                l_freq=self.l_freq,
                h_freq=self.h_freq,
                method=self.method
            )
        
        return X_filt
```

---

## å®Œæ•´å·¥ä½œæµç¤ºä¾‹

### ç¤ºä¾‹: Motor Imagery åˆ†ç±»

```python
import mne
from mne.decoding import CSP, Scaler, Vectorizer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import cross_val_score, StratifiedKFold

# 1. åŠ è½½æ•°æ®
epochs = mne.read_epochs('motor_imagery_epochs-epo.fif')

# 2. å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
X = epochs.get_data()  # (n_epochs, n_channels, n_times)
y = epochs.events[:, -1]  # å·¦æ‰‹ vs å³æ‰‹

# 3. æ„å»º Pipeline
pipeline = make_pipeline(
    # Step 1: æ ‡å‡†åŒ– (ä¿ç•™ 3D)
    Scaler(epochs.info, scalings='mean'),
    
    # Step 2: CSP ç©ºé—´æ»¤æ³¢
    CSP(n_components=4, reg='ledoit_wolf', log=True),
    
    # Step 3: LDA åˆ†ç±»å™¨
    LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')
)

# 4. äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(
    pipeline, X, y,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1
)

print(f"åˆ†ç±»å‡†ç¡®ç‡: {scores.mean():.2%} Â± {scores.std():.2%}")

# 5. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
pipeline.fit(X, y)

# 6. æå– CSP æ¨¡å¼
csp = pipeline.named_steps['csp']
patterns = csp.patterns_

# 7. å¯è§†åŒ–ç©ºé—´æ¨¡å¼
import matplotlib.pyplot as plt
from mne.viz import plot_topomap

fig, axes = plt.subplots(1, 4, figsize=(12, 3))
for i in range(4):
    plot_topomap(
        patterns[:, i], 
        epochs.info, 
        axes=axes[i],
        show=False
    )
    axes[i].set_title(f'CSP {i+1}')

plt.tight_layout()
plt.show()
```

---

## æ€»ç»“

### scikit-learn åœ¨ MNE ä¸­çš„ä»·å€¼

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **è§£ç åˆ†æ** | â­â­â­â­â­ | `mne.decoding` æ ¸å¿ƒä¾èµ– |
| **æœºå™¨å­¦ä¹ ** | â­â­â­â­â­ | åˆ†ç±»ã€å›å½’æ ‡å‡†æ¥å£ |
| **Pipeline** | â­â­â­â­â­ | å·¥ä½œæµç»„åˆå…³é”® |
| **äº¤å‰éªŒè¯** | â­â­â­â­â­ | æ¨¡å‹è¯„ä¼°å¿…å¤‡ |
| **å¯æ›¿ä»£æ€§** | â­â­â­ | å¯ç”¨å…¶ä»– ML åº“ï¼Œä½†éœ€é€‚é… |

---

### MNE å¯¹ sklearn çš„æ‰©å±•

1. **ä¿ç•™æ•°æ®ç»´åº¦**: Scaler, Vectorizer å¤„ç† 3D epochs
2. **æ—¶é—´è§£ç **: SlidingEstimator, GeneralizingEstimator
3. **ç¥ç»ç§‘å­¦ç‰¹å®š**: CSP, SSD, Xdawn
4. **æ— ç¼é›†æˆ**: éµå¾ª sklearn API æ ‡å‡†

---

**è¿”å›**: [ä¾èµ–åˆ†ææ€»è§ˆ](dependency-analysis-overview.md)  
**ä¸Šä¸€æ­¥**: [SciPy ä¾èµ–åˆ†æ](dependency-scipy.md)
