# MNE-ICALabel æ·±åº¦å­¦ä¹ åç«¯æ¶æ„è¯¦è§£

> **ç”¨é€”**: è¯¦ç»†æ‹†è§£ MNE-ICALabel çš„æ¨ç†æµç¨‹å’Œåç«¯æ¶æ„  
> **åˆ›å»ºæ—¥æœŸ**: 2026-01-31  
> **æ ¸å¿ƒ**: ä»ç‰¹å¾æå–åˆ°æ¨¡å‹æ¨ç†çš„å®Œæ•´æ•°æ®æµ

---

## ç›®å½•

1. [æ•°æ®æ ¼å¼å…¼å®¹æ€§](#æ•°æ®æ ¼å¼å…¼å®¹æ€§)
2. [å®Œæ•´æ•°æ®æµ](#å®Œæ•´æ•°æ®æµ)
3. [ç‰¹å¾æå–è¯¦è§£](#ç‰¹å¾æå–è¯¦è§£)
4. [æ¨¡å‹è¾“å…¥æ ¼å¼](#æ¨¡å‹è¾“å…¥æ ¼å¼)
5. [ç¥ç»ç½‘ç»œæ¶æ„](#ç¥ç»ç½‘ç»œæ¶æ„)
6. [ç°æœ‰åç«¯å®ç°](#ç°æœ‰åç«¯å®ç°)
7. [ç†è®ºä¸Šå¯æ·»åŠ çš„åç«¯](#ç†è®ºä¸Šå¯æ·»åŠ çš„åç«¯)
8. [Rust åç«¯å¯è¡Œæ€§åˆ†æ](#rust-åç«¯å¯è¡Œæ€§åˆ†æ)

---

## æ•°æ®æ ¼å¼å…¼å®¹æ€§

### .fif æ ¼å¼å¯ä»¥å®Œå…¨æ›¿æ¢ï¼

**é‡è¦**: MNE-ICALabel **ä¸ä¾èµ–** `.fif` æ ¼å¼ï¼Œåªéœ€è¦ `mne.io.Raw` æˆ– `mne.Epochs` å¯¹è±¡ã€‚

```python
# âœ… è¿™äº›æ ¼å¼éƒ½å¯ä»¥!
from mne_icalabel import label_components

# æ–¹æ³• 1: XDF (LSL å½•åˆ¶æ ¼å¼)
raw = mne.io.read_raw_xdf('recording.xdf')
ic_labels = label_components(raw, ica, method='iclabel')

# æ–¹æ³• 2: EDF (æ¬§æ´²æ ‡å‡†æ ¼å¼)
raw = mne.io.read_raw_edf('data.edf')
ic_labels = label_components(raw, ica, method='iclabel')

# æ–¹æ³• 3: BrainVision
raw = mne.io.read_raw_brainvision('data.vhdr')
ic_labels = label_components(raw, ica, method='iclabel')

# æ–¹æ³• 4: LSL å®æ—¶æµ (MNE-LSL)
from mne_lsl.stream import StreamLSL
stream = StreamLSL(bufsize=10, name='YourStream')
stream.connect()
ic_labels = label_components(stream, ica, method='iclabel')  # âœ… ç›´æ¥æ”¯æŒ!
```

---

### XDF æ ¼å¼ (LSL å½•åˆ¶) - æ‚¨çš„åœºæ™¯

#### ä»€ä¹ˆæ˜¯ XDFï¼Ÿ

```
XDF (eXtensible Data Format)
â”œâ”€â”€ Lab Streaming Layer çš„å½•åˆ¶æ ¼å¼
â”œâ”€â”€ å­˜å‚¨å¤šæµæ•°æ® + æ—¶é—´æˆ³åŒæ­¥
â””â”€â”€ æ–‡ä»¶æ‰©å±•å: .xdf
```

#### å®Œæ•´ç¤ºä¾‹: XDF â†’ ICALabel

```python
import mne
from mne.preprocessing import ICA
from mne_icalabel import label_components

# ========================================
# 1. è¯»å– LSL å½•åˆ¶çš„ XDF æ–‡ä»¶
# ========================================
# å®‰è£…: pip install pyxdf
raw = mne.io.read_raw_xdf(
    'my_lsl_recording.xdf',
    stream_ids=[0],  # é€‰æ‹©ç¬¬ä¸€ä¸ªæµ (å¦‚æœæœ‰å¤šä¸ª)
    preload=True
)

print(f"âœ… è¯»å–æˆåŠŸ: {len(raw.ch_names)} é€šé“, {raw.times[-1]:.1f} ç§’")

# ========================================
# 2. é¢„å¤„ç† (ICLabel è¦æ±‚)
# ========================================
# æ»¤æ³¢: 1-100 Hz
raw.filter(l_freq=1.0, h_freq=100.0)

# è®¾ç½®å‚è€ƒ (å¦‚æœæ˜¯ EEG)
if 'eeg' in raw.get_channel_types():
    raw.set_eeg_reference('average')

# ========================================
# 3. ICA åˆ†è§£
# ========================================
ica = ICA(
    n_components=15,
    method='infomax',
    fit_params=dict(extended=True),
    random_state=42
)
ica.fit(raw)

# ========================================
# 4. è‡ªåŠ¨åˆ†ç±» (ä¸ .fif å®Œå…¨ç›¸åŒ!)
# ========================================
ic_labels = label_components(raw, ica, method='iclabel')

print("\næˆåˆ†åˆ†ç±»ç»“æœ:")
for i, (label, prob) in enumerate(zip(ic_labels['labels'], 
                                       ic_labels['y_pred_proba'])):
    print(f"  ICA{i:02d}: {label:20s} ({prob*100:.1f}%)")

# æ’é™¤ä¼ªè¿¹
exclude_idx = [i for i, label in enumerate(ic_labels['labels'])
               if label not in ['brain', 'other']]
ica.apply(raw, exclude=exclude_idx)

print(f"\nâœ… æ’é™¤ {len(exclude_idx)} ä¸ªä¼ªè¿¹æˆåˆ†")
```

#### XDF å¤šæµå¤„ç†

```python
import pyxdf

# æŸ¥çœ‹ XDF æ–‡ä»¶åŒ…å«å“ªäº›æµ
streams, header = pyxdf.load_xdf('recording.xdf')

print("XDF æ–‡ä»¶åŒ…å«çš„æµ:")
for i, stream in enumerate(streams):
    print(f"  Stream {i}: {stream['info']['name'][0]}")
    print(f"    ç±»å‹: {stream['info']['type'][0]}")
    print(f"    é€šé“æ•°: {stream['info']['channel_count'][0]}")

# è¯»å–æŒ‡å®šæµ
raw = mne.io.read_raw_xdf(
    'recording.xdf',
    stream_ids=[0],  # EEG æµ
    preload=True
)
```

---

### EDF æ ¼å¼ - åŒ»ç–—æ ‡å‡†

#### å®Œæ•´ç¤ºä¾‹: EDF â†’ ICALabel

```python
import mne
from mne.preprocessing import ICA
from mne_icalabel import label_components

# ========================================
# 1. è¯»å– EDF æ–‡ä»¶
# ========================================
raw = mne.io.read_raw_edf(
    'patient_data.edf',
    preload=True,
    stim_channel='auto'  # è‡ªåŠ¨æ£€æµ‹åˆºæ¿€é€šé“
)

# EDF æ–‡ä»¶ä¿¡æ¯
print(f"é‡‡æ ·ç‡: {raw.info['sfreq']} Hz")
print(f"é€šé“: {raw.ch_names}")

# ========================================
# 2. é€‰æ‹© EEG é€šé“ (EDF å¯èƒ½åŒ…å«å…¶ä»–ç±»å‹)
# ========================================
raw.pick_types(eeg=True, exclude=[])

# ========================================
# 3. é¢„å¤„ç†
# ========================================
raw.filter(1.0, 100.0)
raw.set_eeg_reference('average')

# ========================================
# 4. ICA + è‡ªåŠ¨åˆ†ç±»
# ========================================
ica = ICA(n_components=15, method='infomax',
          fit_params=dict(extended=True))
ica.fit(raw)

ic_labels = label_components(raw, ica, method='iclabel')

# æ’é™¤ä¼ªè¿¹
exclude_idx = [i for i, label in enumerate(ic_labels['labels'])
               if label not in ['brain', 'other']]
ica.apply(raw, exclude=exclude_idx)

# ========================================
# 5. ä¿å­˜æ¸…æ´—åçš„æ•°æ® (å¯é€‰)
# ========================================
# ä¿å­˜ä¸º EDF
mne.export.export_raw('cleaned_data.edf', raw, fmt='edf', overwrite=True)

# æˆ–ä¿å­˜ä¸º FIFF (MNE åŸç”Ÿï¼Œæ›´å¿«)
raw.save('cleaned_data.fif', overwrite=True)
```

#### EDF vs BDF (é‡è¦åŒºåˆ«)

```python
# âŒ EDF: 16-bit (æ ‡å‡† EEG,ç²¾åº¦æœ‰é™)
raw = mne.io.read_raw_edf('data.edf')
# ç²¾åº¦: Â±327.68 mV (16-bit æ•´æ•°)
# é‡‡æ ·ç‡: é€šå¸¸ â‰¤ 256 Hz
# é€‚ç”¨: ä¸´åºŠ EEG,ç¡çœ ç ”ç©¶

# âœ… BDF: 24-bit (BioSemi æ ¼å¼,é«˜ç²¾åº¦)
raw = mne.io.read_raw_bdf('data.bdf')
# ç²¾åº¦: Â±8388.608 mV (24-bit æ•´æ•°,256x æ›´ç²¾ç¡®)
# é‡‡æ ·ç‡: å¯è¾¾ 16 kHz+
# é€‚ç”¨: ç ”ç©¶çº§ EEG/ERP

# ä½¿ç”¨æ–¹å¼å®Œå…¨ç›¸åŒ
ic_labels = label_components(raw, ica, method='iclabel')
```

**å…³é”®åŒºåˆ«**:

| ç‰¹æ€§ | EDF | BDF |
|------|-----|-----|
| **ä½æ·±åº¦** | 16-bit | **24-bit** âœ… |
| **ç²¾åº¦** | Â±32768 levels | Â±8388608 levels (256x) |
| **åŠ¨æ€èŒƒå›´** | ~96 dB | ~144 dB |
| **æ–‡ä»¶å¤§å°** | è¾ƒå° | å¤§ 50% |
| **æ ‡å‡†** | æ¬§æ´²åŒ»ç–—æ ‡å‡† (1992) | BioSemi æ‰©å±• (2003) |
| **æ¨èç”¨äº** | ä¸´åºŠ,é•¿æ—¶ç¨‹å½•åˆ¶ | ç ”ç©¶,é«˜ç²¾åº¦åˆ†æ |

**å¼€æºå®ç°** âœ…:

| åº“/å·¥å…· | BDF æ”¯æŒ | EDF æ”¯æŒ | è®¸å¯è¯ |
|---------|----------|----------|--------|
| **MNE-Python** | âœ… `mne.io.read_raw_bdf()` | âœ… `mne.io.read_raw_edf()` | BSD-3 |
| **pyEDFlib** | âœ… å®Œæ•´æ”¯æŒ | âœ… å®Œæ•´æ”¯æŒ | BSD-2 |
| **EDFbrowser** | âœ… å¯è§†åŒ– | âœ… å¯è§†åŒ– | GPL-3 |
| **BioSig** | âœ… C/C++/Octave | âœ… å®Œæ•´æ”¯æŒ | GPL-3 |
| **EEGLAB (MATLAB)** | âœ… biosig æ’ä»¶ | âœ… åŸç”Ÿæ”¯æŒ | GPL-2 |

**å…¼å®¹æ€§** ğŸ”„:

```python
# ========================================
# BDF â†’ EDF è½¬æ¢ (å‘ä¸‹å…¼å®¹,æŸå¤±ç²¾åº¦)
# ========================================
import mne

# è¯»å– BDF (24-bit)
raw_bdf = mne.io.read_raw_bdf('high_precision.bdf', preload=True)

# å¯¼å‡ºä¸º EDF (è‡ªåŠ¨é™é‡‡æ ·åˆ° 16-bit)
mne.export.export_raw('converted.edf', raw_bdf, fmt='edf')
# âš ï¸ è­¦å‘Š: 24-bit â†’ 16-bit ä¼šæŸå¤±ç²¾åº¦!

# ========================================
# EDF â†’ BDF è½¬æ¢ (å‘ä¸Šå…¼å®¹,æ— æŸ)
# ========================================
raw_edf = mne.io.read_raw_edf('standard.edf', preload=True)

# EDF æ•°æ®å¯ä»¥åœ¨ BDF ç³»ç»Ÿä¸­ä½¿ç”¨ (æ— éœ€è½¬æ¢)
# BDF è¯»å–å™¨é€šå¸¸ä¹Ÿèƒ½è¯» EDF æ–‡ä»¶

# å¦‚éœ€ä¿å­˜ä¸º BDF æ ¼å¼:
import pyedflib
n_channels = len(raw_edf.ch_names)
signals = raw_edf.get_data() * 1e6  # è½¬æ¢ä¸º ÂµV

with pyedflib.EdfWriter('converted.bdf', n_channels, file_type=pyedflib.FILETYPE_BDFPLUS) as f:
    channel_info = []
    for ch_name in raw_edf.ch_names:
        ch_dict = {
            'label': ch_name,
            'dimension': 'uV',
            'sample_rate': raw_edf.info['sfreq'],
            'physical_max': signals.max(),
            'physical_min': signals.min(),
            'digital_max': 8388607,   # 24-bit max
            'digital_min': -8388608,  # 24-bit min
        }
        channel_info.append(ch_dict)
    
    f.setSignalHeaders(channel_info)
    f.writeSamples(signals)
```

**æ ¼å¼å…¼å®¹æ€§æ€»ç»“**:

| åœºæ™¯ | å…¼å®¹æ€§ | è¯´æ˜ |
|------|--------|------|
| **BDF è¯»å–å™¨è¯» EDF** | âœ… å®Œå…¨å…¼å®¹ | BDF æ˜¯ EDF çš„è¶…é›† |
| **EDF è¯»å–å™¨è¯» BDF** | âš ï¸ éƒ¨åˆ†å…¼å®¹ | æ—§ç‰ˆ EDF è½¯ä»¶å¯èƒ½ä¸æ”¯æŒ 24-bit |
| **BDF â†’ EDF è½¬æ¢** | âš ï¸ æŸå¤±ç²¾åº¦ | 24-bit â†’ 16-bit æˆªæ–­ |
| **EDF â†’ BDF è½¬æ¢** | âœ… æ— æŸ | 16-bit æ•°æ®åœ¨ 24-bit å®¹å™¨ä¸­ |
| **MNE-Python** | âœ… å®Œå…¨å…¼å®¹ | ä¸¤ç§æ ¼å¼ä½¿ç”¨ç›¸åŒ API |

**æ‚¨çš„åœºæ™¯æ¨è**:
- å¦‚æœ LSL é‡‡é›†è®¾å¤‡æ”¯æŒé«˜ç²¾åº¦ â†’ ä¿å­˜ä¸º **BDF** âœ…
- å¦‚æœéœ€è¦å…¼å®¹æ€§/æ–‡ä»¶å° â†’ ä¿å­˜ä¸º **EDF** âš ï¸

---

### LSL å®æ—¶æµ â†’ ICALabel

#### åœºæ™¯ 1: ç¦»çº¿åˆ†æ LSL å½•åˆ¶æ•°æ®

```python
# æ‚¨çš„åœºæ™¯: LSL å½•åˆ¶ â†’ XDF â†’ ç¦»çº¿ ICA
from mne_icalabel import label_components

# 1. è¯»å– LSL å½•åˆ¶çš„ XDF
raw = mne.io.read_raw_xdf('lsl_recording.xdf', preload=True)

# 2. ç¦»çº¿ ICA åˆ†æ
raw.filter(1, 100).set_eeg_reference('average')
ica = ICA(n_components=15, method='infomax', 
          fit_params=dict(extended=True))
ica.fit(raw)

# 3. è‡ªåŠ¨åˆ†ç±»
ic_labels = label_components(raw, ica, method='iclabel')

# 4. åº”ç”¨æ¸…æ´—
exclude = [i for i, l in enumerate(ic_labels['labels']) 
           if l not in ['brain', 'other']]
ica.apply(raw, exclude=exclude)
```

#### åœºæ™¯ 2: å‡†å®æ—¶ ICA (1ç§’å»¶è¿Ÿå¯æ¥å—)

**æ‚¨çš„åœºæ™¯**: 1ç§’å»¶è¿Ÿå¯æ¥å— â†’ **å®Œå…¨å¯è¡Œ**!

**æ–¹æ¡ˆ A: é¢„è®­ç»ƒ ICA + å®æ—¶åº”ç”¨** (æ¨è)

```python
# --- æ­¥éª¤ 1: ç¦»çº¿è®­ç»ƒ ICA (ä¸€æ¬¡æ€§,5åˆ†é’Ÿ) ---
import mne
from mne.preprocessing import ICA
from mne_icalabel import label_components

# æ”¶é›†æ ¡å‡†æ•°æ® (å¯ä»¥æ˜¯ XDF å½•åˆ¶)
raw = mne.io.read_raw_xdf('calibration_5min.xdf', preload=True)
raw.filter(1, 100).set_eeg_reference('average')

ica = ICA(n_components=15, method='infomax',
          fit_params=dict(extended=True))
ica.fit(raw)

ic_labels = label_components(raw, ica, method='iclabel')
exclude = [i for i, l in enumerate(ic_labels['labels'])
           if l not in ['brain', 'other']]
ica.exclude = exclude

# ä¿å­˜æ¨¡å‹
ica.save('trained_ica.fif')
print(f"âœ… è®­ç»ƒå®Œæˆ,æ’é™¤æˆåˆ†: {exclude}")

# --- æ­¥éª¤ 2: å®æ—¶åº”ç”¨ (ä½å»¶è¿Ÿ ~100ms) ---
from mne_lsl.stream import StreamLSL
from mne.preprocessing import read_ica
import numpy as np
import time

# åŠ è½½æ¨¡å‹
ica = read_ica('trained_ica.fif')

# è¿æ¥å®æ—¶æµ
stream = StreamLSL(bufsize=2, name='MyEEG')
stream.connect()
stream.filter(1, 100, phase='minimum')
stream.set_eeg_reference('average')

print("ğŸš€ å®æ—¶ ICA æ¸…æ´—å¯åŠ¨ (å»¶è¿Ÿ ~100ms)")

while True:
    # è·å–æœ€æ–° 0.5 ç§’æ•°æ®
    data, times = stream.get_data(winsize=0.5)
    
    # å¿«é€Ÿ ICA åº”ç”¨ (~10ms)
    sources = np.dot(ica.unmixing_matrix_, data)
    sources[ica.exclude, :] = 0  # ç§»é™¤ä¼ªè¿¹
    data_clean = np.dot(ica.mixing_matrix_, sources)
    
    # âœ… æ€»å»¶è¿Ÿ: ~100ms (è¿œä½äºæ‚¨çš„ 1ç§’è¦æ±‚)
    
    # æ‚¨çš„å®æ—¶å¤„ç†
    # process_realtime(data_clean)
    
    time.sleep(0.1)  # 100ms æ›´æ–°
from mne.io import RawArray
info = stream.info
raw_segment = RawArray(data, info)

# 5. ICA åˆ†æ
raw_segment.filter(1, 100)
ica = ICA(n_components=15, method='infomax',
          fit_params=dict(extended=True))
ica.fit(raw_segment)

# 6. è‡ªåŠ¨åˆ†ç±»
ic_labels = label_components(raw_segment, ica, method='iclabel')

# 7. åº”ç”¨åˆ°å®æ—¶æµ
# âš ï¸ MNE-LSL çš„ StreamLSL ä¸ç›´æ¥æ”¯æŒ ICA.apply()
# éœ€è¦æ‰‹åŠ¨å¤„ç†æˆ–åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼
```

**å®æ—¶ ICA çš„å±€é™**:
- âŒ ICA æ‹Ÿåˆéœ€è¦å¤§é‡æ•°æ® (å»ºè®® >30 ç§’)
- âŒ æ— æ³•åœ¨"çœŸæ­£å®æ—¶"ä¸­é‡æ–°è®­ç»ƒ ICA
- âœ… å¯è¡Œæ–¹æ¡ˆ: ç¦»çº¿è®­ç»ƒ ICA â†’ ä¿å­˜ â†’ å®æ—¶åº”ç”¨

#### æ¨èå·¥ä½œæµ: ç¦»çº¿ ICA + å®æ—¶åº”ç”¨

```python
# ========================================
# é˜¶æ®µ 1: ç¦»çº¿è®­ç»ƒ ICA (ä½¿ç”¨å½•åˆ¶æ•°æ®)
# ========================================
import mne
from mne.preprocessing import ICA
from mne_icalabel import label_components

# 1. è¯»å– XDF å½•åˆ¶
raw = mne.io.read_raw_xdf('calibration.xdf', preload=True)
raw.filter(1, 100).set_eeg_reference('average')

# 2. è®­ç»ƒ ICA
ica = ICA(n_components=15, method='infomax',
          fit_params=dict(extended=True))
ica.fit(raw)

# 3. è‡ªåŠ¨åˆ†ç±»
ic_labels = label_components(raw, ica, method='iclabel')
exclude = [i for i, l in enumerate(ic_labels['labels']) 
           if l not in ['brain', 'other']]

# 4. ä¿å­˜ ICA (å«æ’é™¤åˆ—è¡¨)
ica.exclude = exclude
ica.save('trained_ica.fif')

print(f"âœ… ICA è®­ç»ƒå®Œæˆï¼Œæ’é™¤æˆåˆ†: {exclude}")

# ========================================
# é˜¶æ®µ 2: å®æ—¶åº”ç”¨ ICA (ä½¿ç”¨ MNE-LSL)
# ========================================
from mne_lsl.stream import StreamLSL
from mne.preprocessing import read_ica

# 1. åŠ è½½è®­ç»ƒå¥½çš„ ICA
ica = read_ica('trained_ica.fif')

# 2. è¿æ¥å®æ—¶æµ
stream = StreamLSL(bufsize=5, name='MyEEG')
stream.connect()
stream.filter(1, 100, phase='minimum')
stream.set_eeg_reference('average')

# 3. å®æ—¶å¾ªç¯
while True:
    # è·å–æœ€æ–°æ•°æ®
    data, times = stream.get_data(winsize=2)
    
    # åº”ç”¨ ICA æ¸…æ´—
    data_clean = ica.apply(data, exclude=ica.exclude)
    
    # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®åšåˆ†æ
    # analyze(data_clean)
```

---

### æ ¼å¼å¯¹æ¯” (é’ˆå¯¹æ‚¨çš„åœºæ™¯)

| æ ¼å¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ | æ¨èåœºæ™¯ |
|------|------|------|------|
| **XDF** | âœ… LSL åŸç”Ÿ<br>âœ… å¤šæµåŒæ­¥<br>âœ… ç²¾ç¡®æ—¶é—´æˆ³<br>âš ï¸ **ç¦»çº¿æ ¼å¼** | âš ï¸ éœ€è¦ pyxdf<br>âš ï¸ æ–‡ä»¶å¯èƒ½å¾ˆå¤§<br>âŒ ä¸æ”¯æŒå®æ—¶ | **LSL å½•åˆ¶æ•°æ®** (ç¦»çº¿åˆ†æ) |
| **EDF** | âœ… åŒ»ç–—æ ‡å‡†<br>âœ… å¹¿æ³›å…¼å®¹<br>âœ… é•¿æ—¶ç¨‹å½•åˆ¶ | âŒ 16-bit ç²¾åº¦<br>âš ï¸ å…ƒæ•°æ®æœ‰é™ | ä¸´åºŠæ•°æ®,ç¡çœ ç ”ç©¶ |
| **BDF** | âœ… **24-bit ç²¾åº¦** ğŸ¯<br>âœ… é«˜é‡‡æ ·ç‡<br>âœ… BioSemi æ ‡å‡† | âš ï¸ æ–‡ä»¶è¾ƒå¤§<br>âš ï¸ å…¼å®¹æ€§ç•¥ä½ | **ç ”ç©¶çº§ EEG** (æ‚¨çš„é«˜ç²¾åº¦åœºæ™¯) |
| **FIFF** | âœ… MNE æœ€å¿«<br>âœ… å…ƒæ•°æ®å®Œæ•´ | âŒ éé€šç”¨æ ¼å¼ | MNE å†…éƒ¨å¤„ç† |
| **LSL Stream** | âœ… **çœŸå®æ—¶** ğŸš€<br>âœ… ä½å»¶è¿Ÿ (<100ms)<br>âœ… å¤šè®¾å¤‡åŒæ­¥ | âŒ ä¸èƒ½ä¿å­˜<br>âš ï¸ éœ€è¦ MNE-LSL | **å®æ—¶ BCI/ç¥ç»åé¦ˆ** (æ‚¨çš„å®æ—¶åœºæ™¯) |

---

### å®Œæ•´å·¥ä½œæµ: XDF â†’ ICALabel â†’ æ¸…æ´—æ•°æ®

```python
import mne
from mne.preprocessing import ICA
from mne_icalabel import label_components

# ========================================
# Step 1: è¯»å– LSL å½•åˆ¶çš„ XDF
# ========================================
print("ğŸ“‚ è¯»å– XDF æ–‡ä»¶...")
raw = mne.io.read_raw_xdf('my_lsl_recording.xdf', preload=True)

print(f"âœ… æ•°æ®ä¿¡æ¯:")
print(f"   é€šé“æ•°: {len(raw.ch_names)}")
print(f"   é‡‡æ ·ç‡: {raw.info['sfreq']} Hz")
print(f"   æ—¶é•¿: {raw.times[-1]:.1f} ç§’")

# ========================================
# Step 2: é¢„å¤„ç†
# ========================================
print("\nğŸ”§ é¢„å¤„ç†...")

# é€‰æ‹© EEG é€šé“ (å¦‚æœæœ‰å…¶ä»–ç±»å‹)
raw.pick_types(eeg=True, exclude=[])

# æ»¤æ³¢: 1-100 Hz (ICLabel è¦æ±‚)
raw.filter(l_freq=1.0, h_freq=100.0)

# å¹³å‡å‚è€ƒ (ICLabel è¦æ±‚)
raw.set_eeg_reference('average')

# ========================================
# Step 3: ICA åˆ†è§£
# ========================================
print("\nğŸ§  è¿è¡Œ ICA...")
ica = ICA(
    n_components=15,
    method='infomax',
    fit_params=dict(extended=True),
    random_state=42,
    max_iter='auto'
)
ica.fit(raw)

print(f"âœ… ICA å®Œæˆ: {ica.n_components_} ä¸ªæˆåˆ†")

# ========================================
# Step 4: è‡ªåŠ¨åˆ†ç±» (ICLabel)
# ========================================
print("\nğŸ¤– è‡ªåŠ¨åˆ†ç±»æˆåˆ†...")
ic_labels = label_components(raw, ica, method='iclabel')

print("\næˆåˆ†åˆ†ç±»ç»“æœ:")
print("="*70)
for i, (label, prob) in enumerate(zip(ic_labels['labels'], 
                                       ic_labels['y_pred_proba'])):
    icon = "âœ…" if label == "brain" else "âŒ"
    print(f"{icon} ICA{i:02d}: {label:20s} (ç½®ä¿¡åº¦: {prob*100:5.1f}%)")

# ========================================
# Step 5: æ’é™¤ä¼ªè¿¹
# ========================================
exclude_idx = [i for i, label in enumerate(ic_labels['labels'])
               if label not in ['brain', 'other']]

print(f"\nğŸ—‘ï¸  æ’é™¤æˆåˆ†: {exclude_idx}")
print(f"   æ ‡ç­¾: {[ic_labels['labels'][i] for i in exclude_idx]}")

# ========================================
# Step 6: åº”ç”¨æ¸…æ´—
# ========================================
raw_clean = raw.copy()
ica.apply(raw_clean, exclude=exclude_idx)

print("\nâœ… ICA æ¸…æ´—å®Œæˆ!")

# ========================================
# Step 7: ä¿å­˜ç»“æœ
# ========================================
# ä¿å­˜ä¸º XDF (å¦‚æœéœ€è¦ä¿æŒæ ¼å¼)
# âš ï¸ MNE ä¸æ”¯æŒå¯¼å‡º XDFï¼Œéœ€è¦å…¶ä»–å·¥å…·

# ä¿å­˜ä¸º FIFF (æ¨èï¼ŒMNE åŸç”Ÿ)
raw_clean.save('cleaned_data.fif', overwrite=True)
print("ğŸ’¾ ä¿å­˜ä¸º: cleaned_data.fif")

# ä¿å­˜ä¸º EDF (å¦‚æœéœ€è¦åŒ»ç–—æ ‡å‡†æ ¼å¼)
mne.export.export_raw('cleaned_data.edf', raw_clean, fmt='edf', overwrite=True)
print("ğŸ’¾ ä¿å­˜ä¸º: cleaned_data.edf")

# ä¿å­˜ ICA æ¨¡å‹ (ç”¨äºåç»­åº”ç”¨)
ica.exclude = exclude_idx
ica.save('trained_ica.fif', overwrite=True)
print("ğŸ’¾ ä¿å­˜ ICA: trained_ica.fif")
```

---

### å¸¸è§é—®é¢˜

#### Q1: XDF è¯»å–æŠ¥é”™ "No module named 'pyxdf'"ï¼Ÿ

```bash
# å®‰è£… pyxdf
pip install pyxdf

# æˆ–ä½¿ç”¨ conda
conda install -c conda-forge pyxdf
```

#### Q2: EDF æ–‡ä»¶é€šé“åä¸è§„èŒƒï¼Ÿ

```python
# è¯»å– EDF
raw = mne.io.read_raw_edf('data.edf', preload=True)

# é‡å‘½åé€šé“
raw.rename_channels({
    'FP1': 'Fp1',  # æ ‡å‡†åŒ–å‘½å
    'FP2': 'Fp2',
    # ...
})

# è®¾ç½®é€šé“ç±»å‹
raw.set_channel_types({
    'EOG1': 'eog',
    'ECG': 'ecg'
})
```

#### Q3: LSL æµæ²¡æœ‰ç”µæä½ç½®ï¼Ÿ

```python
# æ‰‹åŠ¨è®¾ç½®æ ‡å‡† 10-20 ä½ç½®
raw = mne.io.read_raw_xdf('recording.xdf')

# ä½¿ç”¨æ ‡å‡† montage
montage = mne.channels.make_standard_montage('standard_1020')
raw.set_montage(montage)

# æˆ–è‡ªå®šä¹‰ä½ç½®
# raw.set_montage(my_custom_montage)
```

#### Q4: èƒ½å¦ç›´æ¥åœ¨ LSL æµä¸Šè¿è¡Œ ICALabelï¼Ÿ

```python
# âœ… å¯ä»¥! MNE-LSL çš„ StreamLSL å…¼å®¹
from mne_lsl.stream import StreamLSL

stream = StreamLSL(bufsize=60, name='MyEEG')
stream.connect()

# ... ç­‰å¾…æ•°æ® ...

# ç›´æ¥ä½¿ç”¨ (StreamLSL ç»§æ‰¿è‡ª BaseRaw)
ic_labels = label_components(stream, ica, method='iclabel')
```

---

### æ€»ç»“

| æ‚¨çš„é—®é¢˜ | ç­”æ¡ˆ |
|---------|------|
| **XDF å¯ä»¥ç”¨å—ï¼Ÿ** | âœ… å®Œå…¨å¯ä»¥! `mne.io.read_raw_xdf()` |
| **EDF å¯ä»¥ç”¨å—ï¼Ÿ** | âœ… å®Œå…¨å¯ä»¥! `mne.io.read_raw_edf()` |
| **LSL æµå¯ä»¥å—ï¼Ÿ** | âœ… å¯ä»¥! MNE-LSL `StreamLSL` å…¼å®¹ |
| **å¿…é¡»è½¬ .fif å—ï¼Ÿ** | âŒ ä¸éœ€è¦ï¼Œç›´æ¥ç”¨åŸæ ¼å¼ |
| **æ€§èƒ½æœ‰å·®å¼‚å—ï¼Ÿ** | âš ï¸ XDF/EDF è¯»å–ç¨æ…¢ï¼Œä½† ICA é€Ÿåº¦ä¸€æ · |

**æ¨èå·¥ä½œæµ** (é’ˆå¯¹æ‚¨çš„ LSL åœºæ™¯):
1. ğŸ“¼ LSL å½•åˆ¶ â†’ XDF æ–‡ä»¶
2. ğŸ“‚ `mne.io.read_raw_xdf()` è¯»å–
3. ğŸ”§ é¢„å¤„ç† (1-100 Hz, å¹³å‡å‚è€ƒ)
4. ğŸ§  ICA åˆ†è§£
5. ğŸ¤– ICALabel è‡ªåŠ¨åˆ†ç±»
6. ğŸ—‘ï¸ æ’é™¤ä¼ªè¿¹
7. ğŸ’¾ ä¿å­˜ç»“æœ (FIFF æˆ– EDF)

---

## å®Œæ•´æ•°æ®æµ

### ç«¯åˆ°ç«¯æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: åŸå§‹æ•°æ®                                             â”‚
â”‚                                                               â”‚
â”‚  raw = mne.io.read_raw_fif('data.fif')                       â”‚
â”‚  ica = ICA(n_components=15, method='infomax')                â”‚
â”‚  ica.fit(raw)                                                â”‚
â”‚                                                               â”‚
â”‚  è¾“å…¥å¯¹è±¡:                                                    â”‚
â”‚  â€¢ Raw/Epochs: (n_channels, n_samples) ç”µå‹æ•°æ®              â”‚
â”‚  â€¢ ICA: åˆ†è§£çŸ©é˜µ (icawinv, weights, sphere)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: ç‰¹å¾æå– (features.py)                              â”‚
â”‚                                                               â”‚
â”‚  topo, psd, autocorr = get_iclabel_features(raw, ica)       â”‚
â”‚                                                               â”‚
â”‚  æå– 3 ç§ç‰¹å¾:                                               â”‚
â”‚  âœ… topo:    (32, 32, 1, n_components)  æ‹“æ‰‘å›¾               â”‚
â”‚  âœ… psd:     (1, 100, 1, n_components)  åŠŸç‡è°±å¯†åº¦           â”‚
â”‚  âœ… autocorr:(1, 100, 1, n_components)  è‡ªç›¸å…³               â”‚
â”‚                                                               â”‚
â”‚  ç‰¹å¾å·¥ç¨‹ç»†èŠ‚ â–¼â–¼â–¼                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: ç‰¹å¾æ ¼å¼åŒ– (network/utils.py)                       â”‚
â”‚                                                               â”‚
â”‚  _format_input(topo, psd, autocorr)                         â”‚
â”‚                                                               â”‚
â”‚  æ•°æ®å¢å¼º (ç¿»è½¬å’Œå–å):                                       â”‚
â”‚  â€¢ topo: [åŸå§‹, -åŸå§‹, æ°´å¹³ç¿»è½¬, -æ°´å¹³ç¿»è½¬] â†’ x4             â”‚
â”‚  â€¢ psd:  å¤åˆ¶ 4 å€ â†’ x4                                      â”‚
â”‚  â€¢ autocorr: å¤åˆ¶ 4 å€ â†’ x4                                  â”‚
â”‚                                                               â”‚
â”‚  è¾“å‡ºå½¢çŠ¶:                                                    â”‚
â”‚  â€¢ topo:    (32, 32, 1, n_components * 4)                   â”‚
â”‚  â€¢ psd:     (1, 100, 1, n_components * 4)                   â”‚
â”‚  â€¢ autocorr:(1, 100, 1, n_components * 4)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: åç«¯ç‰¹å®šæ ¼å¼åŒ–                                       â”‚
â”‚                                                               â”‚
â”‚  PyTorch:  transpose(3,2,0,1) â†’ to_tensor()                 â”‚
â”‚  ONNX:     transpose(3,2,0,1) â†’ astype(float32)             â”‚
â”‚                                                               â”‚
â”‚  æœ€ç»ˆè¾“å…¥å½¢çŠ¶ (batch-first):                                  â”‚
â”‚  â€¢ topo:    (n_comp*4, 1, 32, 32)   [NCHW]                  â”‚
â”‚  â€¢ psd:     (n_comp*4, 1, 1, 100)   [NCHW]                  â”‚
â”‚  â€¢ autocorr:(n_comp*4, 1, 1, 100)   [NCHW]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: ç¥ç»ç½‘ç»œæ¨ç† (network/torch.py or onnx.py)         â”‚
â”‚                                                               â”‚
â”‚  ICLabelNet(topo, psd, autocorr)                            â”‚
â”‚                                                               â”‚
â”‚  ç½‘ç»œç»“æ„:                                                    â”‚
â”‚  â€¢ æ‹“æ‰‘åˆ†æ”¯:  Conv2D (1â†’128â†’256â†’512) â†’ 512Ã—4Ã—4             â”‚
â”‚  â€¢ PSD åˆ†æ”¯:  Conv2D (1â†’128â†’256â†’1) â†’ 1Ã—1Ã—100 â†’ é‡å¡‘        â”‚
â”‚  â€¢ è‡ªç›¸å…³åˆ†æ”¯: Conv2D (1â†’128â†’256â†’1) â†’ 1Ã—1Ã—100 â†’ é‡å¡‘       â”‚
â”‚  â€¢ åˆå¹¶: Concat â†’ Conv2D (712â†’7) â†’ Softmax                  â”‚
â”‚                                                               â”‚
â”‚  è¾“å‡ºå½¢çŠ¶: (n_components*4, 7) æ¦‚ç‡åˆ†å¸ƒ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: åå¤„ç†                                               â”‚
â”‚                                                               â”‚
â”‚  å¹³å‡ 4 ä¸ªå¢å¼ºæ ·æœ¬çš„é¢„æµ‹:                                     â”‚
â”‚  labels = reshape(labels, [n_components, 4, 7])             â”‚
â”‚  labels = mean(labels, axis=1)  # (n_components, 7)         â”‚
â”‚                                                               â”‚
â”‚  æœ€ç»ˆè¾“å‡º:                                                    â”‚
â”‚  array([[0.85, 0.02, 0.05, 0.01, 0.01, 0.03, 0.03],  # IC0  â”‚
â”‚         [0.12, 0.78, 0.03, 0.02, 0.01, 0.02, 0.02],  # IC1  â”‚
â”‚         ...])                                                â”‚
â”‚                                                               â”‚
â”‚  7 åˆ—å¯¹åº”: [brain, muscle, eye, heart, line, ch_noise, other]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ç‰¹å¾æå–è¯¦è§£

### 1. æ‹“æ‰‘å›¾ç‰¹å¾ (Topoplot)

**ç›®çš„**: æ•æ‰ ICA æˆåˆ†çš„ç©ºé—´åˆ†å¸ƒæ¨¡å¼

```python
def _eeg_topoplot(inst, icawinv, picks):
    """
    å°† ICA åˆ†è§£çŸ©é˜µè½¬æ¢ä¸º 32Ã—32 åƒç´ çš„æ‹“æ‰‘å›¾
    
    è¾“å…¥:
    - icawinv: (n_channels, n_components) ICA é€†çŸ©é˜µ
    - inst: Raw/Epochs å¯¹è±¡ï¼Œç”¨äºè·å–ç”µæä½ç½®
    
    è¾“å‡º:
    - topo: (32, 32, 1, n_components) float32
    """
    
    # æ­¥éª¤ 1: è·å–ç”µæä½ç½® (æåæ ‡)
    rd, th = _mne_to_eeglab_locs(inst, picks)
    # rd: å¾„å‘è·ç¦» [0, 1]
    # th: è§’åº¦ (åº¦)
    
    # æ­¥éª¤ 2: è½¬æ¢ä¸ºç¬›å¡å°”åæ ‡
    th_rad = th * np.pi / 180
    x, y = pol2cart(th_rad, rd)
    
    # æ­¥éª¤ 3: å¯¹æ¯ä¸ªæˆåˆ†è¿›è¡Œæ’å€¼
    for i in range(n_components):
        values = icawinv[:, i]  # è¯¥æˆåˆ†åœ¨å„ç”µæçš„æƒé‡
        
        # ä½¿ç”¨ griddata (v4) æ’å€¼åˆ° 32Ã—32 ç½‘æ ¼
        topo[:, :, 0, i] = _gdatav4(x, y, values, 32, 32)
        
        # å½’ä¸€åŒ–: é™¤ä»¥æœ€å¤§ç»å¯¹å€¼
        topo[:, :, 0, i] /= np.max(np.abs(topo[:, :, 0, i]))
    
    # æ­¥éª¤ 4: é®è”½å¤´å¤–åŒºåŸŸ (è®¾ä¸º NaN)
    mask = np.sqrt(x**2 + y**2) <= 0.5
    topo[~mask] = np.nan
    
    # æ­¥éª¤ 5: NaN â†’ 0
    topo = np.nan_to_num(topo)
    
    return topo.astype(np.float32)
```

**ç¤ºä¾‹å¯è§†åŒ–**:

```
çœ¼ç”µæˆåˆ†æ‹“æ‰‘å›¾ (32Ã—32):        å¿ƒç”µæˆåˆ†æ‹“æ‰‘å›¾ (32Ã—32):
  å‰                              å‰
  â†‘                               â†‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           Â·Â·Â·Â·Â·
â–ˆâ–ˆâ–ˆâ—‹â–ˆâ–ˆâ–ˆ  â† å‰é¢å¼ºä¿¡å·             Â·Â·â—‹Â·Â·  â† ä¸­å¤®å¼ºä¿¡å·
Â·Â·Â·Â·Â·                           Â·Â·Â·Â·Â·
  å                              å
```

---

### 2. åŠŸç‡è°±å¯†åº¦ç‰¹å¾ (PSD)

**ç›®çš„**: æ•æ‰ ICA æˆåˆ†çš„é¢‘ç‡ç‰¹æ€§

```python
def _eeg_rpsd(inst, ica, icaact):
    """
    è®¡ç®— ICA æˆåˆ†çš„åŠŸç‡è°±å¯†åº¦
    
    è¾“å…¥:
    - icaact: (n_components, n_samples) ICA æ¿€æ´»æ—¶é—´åºåˆ—
    - inst: Raw/Epochs å¯¹è±¡
    
    è¾“å‡º:
    - psd: (1, 100, 1, n_components) float32
    """
    
    # å¸¸é‡
    sfreq = inst.info['sfreq']
    nyquist = int(sfreq / 2)
    nfreqs = min(nyquist, 100)  # é¢‘ç‡ç‚¹æ•°
    
    # æ­¥éª¤ 1: åˆ†çª— (Hamming window)
    n_points = min(icaact.shape[1], int(sfreq))  # çª—é•¿ = 1 ç§’
    window = np.hamming(n_points)
    
    # æ­¥éª¤ 2: è®¡ç®—é‡å çª—å£çš„ç´¢å¼•
    hop_size = n_points // 2  # 50% é‡å 
    n_segments = (icaact.shape[1] - n_points) // hop_size + 1
    
    # æ­¥éª¤ 3: å¯¹æ¯ä¸ªçª—å£è®¡ç®— FFT
    psd_all_segments = []
    for seg_idx in range(n_segments):
        start = seg_idx * hop_size
        end = start + n_points
        segment = icaact[:, start:end] * window
        
        # FFT
        fft_result = np.fft.fft(segment, axis=1)
        psd_segment = np.abs(fft_result[:, :nfreqs])**2
        psd_all_segments.append(psd_segment)
    
    # æ­¥éª¤ 4: ä¸­ä½æ•° PSD (é²æ£’ä¼°è®¡)
    psd = np.median(psd_all_segments, axis=0)
    
    # æ­¥éª¤ 5: å½’ä¸€åŒ–
    # è®¡ç®—æ€»åŠŸç‡
    total_power = np.sum(psd, axis=1, keepdims=True)
    psd = psd / total_power
    
    # æ­¥éª¤ 6: å¯¹æ•°å˜æ¢
    psd = 10 * np.log10(psd + 1e-10)
    
    # æ­¥éª¤ 7: Resample åˆ°æ°å¥½ 100 ä¸ªé¢‘ç‡ç‚¹
    if psd.shape[1] != 100:
        psd = resample_poly(psd, up=100, down=psd.shape[1], axis=1)
    
    # æ­¥éª¤ 8: é‡å¡‘ä¸º (1, 100, 1, n_components)
    psd = psd.T.reshape(1, 100, 1, -1)
    
    return psd.astype(np.float32)
```

**é¢‘ç‡ç‰¹å¾ç¤ºä¾‹**:

```
çœ¼ç”µ PSD (ä½é¢‘ä¸ºä¸»):        å·¥é¢‘å™ªå£° PSD (50Hz å°–å³°):
Power                       Power
  â–²                           â–²
  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       â”‚    Â·
  â”‚â–ˆâ–ˆâ–ˆâ–ŒÂ·Â·                      â”‚    Â·
  â”‚â–ˆâ–ˆâ–ŒÂ·Â·Â·                      â”‚    â–ˆ  â† 50 Hz
  â”‚â–ˆâ–ŒÂ·Â·Â·Â·                      â”‚    Â·
  â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ Freq (Hz)         â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ Freq (Hz)
  0      100                   0      100
```

---

### 3. è‡ªç›¸å…³ç‰¹å¾ (Autocorrelation)

**ç›®çš„**: æ•æ‰ ICA æˆåˆ†çš„æ—¶é—´è§„å¾‹æ€§

```python
def _eeg_autocorr_welch(inst, ica, icaact):
    """
    è®¡ç®— ICA æˆåˆ†çš„è‡ªç›¸å…³å‡½æ•°
    
    è¾“å…¥:
    - icaact: (n_components, n_samples) ICA æ¿€æ´»
    
    è¾“å‡º:
    - autocorr: (1, 100, 1, n_components) float32
    """
    
    # æ­¥éª¤ 1: å¯¹æ¯ä¸ªæˆåˆ†è®¡ç®—è‡ªç›¸å…³
    n_components = icaact.shape[0]
    n_samples = icaact.shape[1]
    autocorr_list = []
    
    for i in range(n_components):
        signal = icaact[i, :]
        
        # å»å‡å€¼
        signal = signal - np.mean(signal)
        
        # æ–¹æ³• 1: FFT å¿«é€Ÿè‡ªç›¸å…³ (Wiener-Khinchin theorem)
        fft_signal = np.fft.fft(signal, n=2*n_samples)
        power_spectrum = np.abs(fft_signal)**2
        autocorr_full = np.fft.ifft(power_spectrum).real
        
        # å–å‰ 100 ä¸ªæ»å
        autocorr_100 = autocorr_full[:100]
        
        # å½’ä¸€åŒ–: é™¤ä»¥é›¶æ»åå€¼
        autocorr_100 = autocorr_100 / autocorr_100[0]
        
        autocorr_list.append(autocorr_100)
    
    # æ­¥éª¤ 2: å †å ä¸ºçŸ©é˜µ
    autocorr = np.array(autocorr_list).T  # (100, n_components)
    
    # æ­¥éª¤ 3: é‡å¡‘ä¸º (1, 100, 1, n_components)
    autocorr = autocorr.reshape(1, 100, 1, -1)
    
    return autocorr.astype(np.float32)
```

**è‡ªç›¸å…³æ¨¡å¼ç¤ºä¾‹**:

```
å¿ƒç”µè‡ªç›¸å…³ (å‘¨æœŸæ€§):        éšæœºå™ªå£°è‡ªç›¸å…³:
   1.0 â–²                       1.0 â–²
       â”‚â–² â–² â–²                       â”‚â–²
   0.5 â”‚ â–¼ â–¼ â–¼                   0.5 â”‚ Â·Â· random Â·Â·
       â”‚  å‘¨æœŸæ€§                     â”‚
   0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ Lag           0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ Lag
       0      100                    0      100
```

---

## æ¨¡å‹è¾“å…¥æ ¼å¼

### å®Œæ•´è¾“å…¥è§„èŒƒ

```python
# ========================================
# ç‰¹å¾æå–åçš„åŸå§‹å½¢çŠ¶
# ========================================
topo_raw    = (32, 32, 1, n_components)  # æ‹“æ‰‘å›¾
psd_raw     = (1, 100, 1, n_components)  # PSD
autocorr_raw = (1, 100, 1, n_components)  # è‡ªç›¸å…³

# ========================================
# æ•°æ®å¢å¼º (_format_input)
# ========================================
# æ‹“æ‰‘å›¾: 4 ç§å˜æ¢
topo_aug = np.concatenate([
    topo_raw,                    # åŸå§‹
    -1 * topo_raw,               # å–å
    np.flip(topo_raw, axis=1),   # æ°´å¹³ç¿»è½¬
    -1 * np.flip(topo_raw, axis=1)  # ç¿»è½¬+å–å
], axis=3)
# å½¢çŠ¶: (32, 32, 1, n_components * 4)

# PSD/Autocorr: ç®€å•å¤åˆ¶ 4 å€
psd_aug = np.tile(psd_raw, (1, 1, 1, 4))
autocorr_aug = np.tile(autocorr_raw, (1, 1, 1, 4))
# å½¢çŠ¶: (1, 100, 1, n_components * 4)

# ========================================
# PyTorch ç‰¹å®šæ ¼å¼åŒ–
# ========================================
# Transpose: (H, W, C, N) â†’ (N, C, H, W)
topo_torch = np.transpose(topo_aug, (3, 2, 0, 1))
psd_torch = np.transpose(psd_aug, (3, 2, 0, 1))
autocorr_torch = np.transpose(autocorr_aug, (3, 2, 0, 1))

# è½¬ä¸º Tensor
topo_tensor = torch.from_numpy(topo_torch).float()
psd_tensor = torch.from_numpy(psd_torch).float()
autocorr_tensor = torch.from_numpy(autocorr_torch).float()

# æœ€ç»ˆå½¢çŠ¶
print(topo_tensor.shape)     # (n_comp*4, 1, 32, 32)
print(psd_tensor.shape)      # (n_comp*4, 1, 1, 100)
print(autocorr_tensor.shape) # (n_comp*4, 1, 1, 100)

# ========================================
# ONNX ç‰¹å®šæ ¼å¼åŒ– (å‡ ä¹ç›¸åŒ)
# ========================================
topo_onnx = np.transpose(topo_aug, (3, 2, 0, 1)).astype(np.float32)
psd_onnx = np.transpose(psd_aug, (3, 2, 0, 1)).astype(np.float32)
autocorr_onnx = np.transpose(autocorr_aug, (3, 2, 0, 1)).astype(np.float32)
```

### ç¤ºä¾‹: 15 ä¸ª IC çš„è¾“å…¥å½¢çŠ¶

```python
n_components = 15

# åŸå§‹ç‰¹å¾
topo:    (32, 32, 1, 15)
psd:     (1, 100, 1, 15)
autocorr:(1, 100, 1, 15)

# æ•°æ®å¢å¼ºå
topo:    (32, 32, 1, 60)  # 15 * 4
psd:     (1, 100, 1, 60)
autocorr:(1, 100, 1, 60)

# Batch-first (PyTorch/ONNX)
topo:    (60, 1, 32, 32)   # [Batch, Channel, Height, Width]
psd:     (60, 1, 1, 100)   # [Batch, Channel, Height, Width]
autocorr:(60, 1, 1, 100)

# æ¨ç†è¾“å‡º
labels:  (60, 7)  # æ¯ä¸ªå¢å¼ºæ ·æœ¬çš„ 7 ç±»æ¦‚ç‡

# åå¤„ç†: å¹³å‡ 4 ä¸ªå¢å¼ºæ ·æœ¬
labels_reshaped = labels.reshape(15, 4, 7)
final_labels = labels_reshaped.mean(axis=1)  # (15, 7)
```

---

## ç¥ç»ç½‘ç»œæ¶æ„

### ICLabelNet å®Œæ•´ç»“æ„

```python
class ICLabelNet(nn.Module):
    """
    ICLabel å·ç§¯ç¥ç»ç½‘ç»œ
    
    è¾“å…¥:
    - topo:    (batch, 1, 32, 32)
    - psd:     (batch, 1, 1, 100)
    - autocorr:(batch, 1, 1, 100)
    
    è¾“å‡º:
    - labels: (batch, 7) Softmax æ¦‚ç‡
    """
    
    def __init__(self):
        super().__init__()
        
        # ========================================
        # åˆ†æ”¯ 1: æ‹“æ‰‘å›¾ (Image) åˆ†æ”¯
        # ========================================
        self.img_conv = nn.Sequential(
            # Conv1: 1 â†’ 128 channels
            nn.Conv2d(1, 128, kernel_size=4, stride=2, padding=1),
            # è¾“å‡º: (batch, 128, 16, 16)
            nn.LeakyReLU(0.2),
            
            # Conv2: 128 â†’ 256 channels
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            # è¾“å‡º: (batch, 256, 8, 8)
            nn.LeakyReLU(0.2),
            
            # Conv3: 256 â†’ 512 channels
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            # è¾“å‡º: (batch, 512, 4, 4)
            nn.LeakyReLU(0.2)
        )
        
        # ========================================
        # åˆ†æ”¯ 2: PSD åˆ†æ”¯
        # ========================================
        self.psds_conv = nn.Sequential(
            # Conv1: 1 â†’ 128 channels (1D Conv on freq axis)
            nn.Conv2d(1, 128, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            # è¾“å‡º: (batch, 128, 1, 100)
            nn.LeakyReLU(0.2),
            
            # Conv2: 128 â†’ 256 channels
            nn.Conv2d(128, 256, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            # è¾“å‡º: (batch, 256, 1, 100)
            nn.LeakyReLU(0.2),
            
            # Conv3: 256 â†’ 1 channel (é™ç»´)
            nn.Conv2d(256, 1, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            # è¾“å‡º: (batch, 1, 1, 100)
            nn.LeakyReLU(0.2)
        )
        
        # ========================================
        # åˆ†æ”¯ 3: Autocorr åˆ†æ”¯ (ä¸ PSD ç›¸åŒç»“æ„)
        # ========================================
        self.autocorr_conv = nn.Sequential(
            nn.Conv2d(1, 128, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            nn.LeakyReLU(0.2),
            nn.Conv2d(256, 1, kernel_size=(1, 3), stride=1, padding=(0, 1)),
            nn.LeakyReLU(0.2)
        )
        # è¾“å‡º: (batch, 1, 1, 100)
        
        # ========================================
        # èåˆå±‚
        # ========================================
        # åˆå¹¶åé€šé“æ•°: 512 + 100 + 100 = 712
        self.final_conv = nn.Conv2d(712, 7, kernel_size=4, stride=1, padding=0)
        # è¾“å‡º: (batch, 7, 1, 1)
        
        self.softmax = nn.Softmax(dim=1)
    
    def reshape_psd_autocorr(self, x):
        """
        PSD/Autocorr é‡å¡‘å’Œæ‰©å±•
        
        è¾“å…¥: (batch, 1, 1, 100)
        è¾“å‡º: (batch, 100, 4, 4)  # åŒ¹é…æ‹“æ‰‘åˆ†æ”¯
        """
        # é‡å¡‘: (batch, 1, 1, 100) â†’ (batch, 100, 1, 1)
        x = x.permute(0, 3, 1, 2)
        
        # å¤åˆ¶æ‰©å±•: (batch, 100, 1, 1) â†’ (batch, 100, 4, 4)
        x = x.repeat(1, 1, 4, 4)
        
        return x
    
    def forward(self, topo, psd, autocorr):
        # ä¸‰ä¸ªåˆ†æ”¯å¹¶è¡Œ
        img_features = self.img_conv(topo)         # (batch, 512, 4, 4)
        psd_features = self.psds_conv(psd)         # (batch, 1, 1, 100)
        autocorr_features = self.autocorr_conv(autocorr)  # (batch, 1, 1, 100)
        
        # PSD/Autocorr é‡å¡‘åŒ¹é…æ‹“æ‰‘å°ºå¯¸
        psd_reshaped = self.reshape_psd_autocorr(psd_features)
        # (batch, 100, 4, 4)
        autocorr_reshaped = self.reshape_psd_autocorr(autocorr_features)
        # (batch, 100, 4, 4)
        
        # é€šé“æ‹¼æ¥
        concat = torch.cat([img_features, psd_reshaped, autocorr_reshaped], dim=1)
        # (batch, 512+100+100=712, 4, 4)
        
        # æœ€ç»ˆåˆ†ç±»
        out = self.final_conv(concat)  # (batch, 7, 1, 1)
        out = out.squeeze(-1).squeeze(-1)  # (batch, 7)
        out = self.softmax(out)  # Softmax å½’ä¸€åŒ–
        
        return out
```

### å‚æ•°é‡ç»Ÿè®¡

```python
# æ‹“æ‰‘åˆ†æ”¯
Conv1: 1Ã—128Ã—4Ã—4   = 2,048 params
Conv2: 128Ã—256Ã—4Ã—4 = 524,288 params
Conv3: 256Ã—512Ã—4Ã—4 = 2,097,152 params

# PSD åˆ†æ”¯
Conv1: 1Ã—128Ã—1Ã—3   = 384 params
Conv2: 128Ã—256Ã—1Ã—3 = 98,304 params
Conv3: 256Ã—1Ã—1Ã—3   = 768 params

# Autocorr åˆ†æ”¯ (åŒ PSD)
                   = 99,456 params

# èåˆå±‚
Conv: 712Ã—7Ã—4Ã—4    = 79,744 params

# æ€»è®¡
Total: ~2.9M parameters
```

---

## ç°æœ‰åç«¯å®ç°

### PyTorch åç«¯

**æ–‡ä»¶**: `mne_icalabel/iclabel/network/torch.py`

```python
def _run_iclabel(images, psds, autocorr):
    """
    PyTorch æ¨ç†æµç¨‹
    """
    # 1. åŠ è½½æ¨¡å‹æƒé‡
    network_file = 'ICLabelNet.pt'
    model = ICLabelNet()
    model.load_state_dict(torch.load(network_file, weights_only=True))
    model.eval()  # è¯„ä¼°æ¨¡å¼
    
    # 2. æ ¼å¼åŒ–è¾“å…¥
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    topo = torch.from_numpy(np.transpose(topo, (3,2,0,1))).float()
    psd = torch.from_numpy(np.transpose(psd, (3,2,0,1))).float()
    autocorr = torch.from_numpy(np.transpose(autocorr, (3,2,0,1))).float()
    
    # 3. æ¨ç† (æ— æ¢¯åº¦)
    with torch.no_grad():
        labels = model(topo, psd, autocorr)
    
    # 4. è½¬å› NumPy
    return labels.numpy()
```

**ç‰¹ç‚¹**:
- âœ… æ”¯æŒ GPU åŠ é€Ÿ (è‡ªåŠ¨æ£€æµ‹ CUDA)
- âœ… åŸç”Ÿ PyTorch æ¨¡å‹ï¼Œé€Ÿåº¦å¿«
- âŒ ä¾èµ–å¤§ (~1GB with CUDA)

---

### ONNX åç«¯

**æ–‡ä»¶**: `mne_icalabel/iclabel/network/onnx.py`

```python
import onnxruntime as ort

def _run_iclabel(images, psds, autocorr):
    """
    ONNX Runtime æ¨ç†æµç¨‹
    """
    # 1. åˆ›å»ºæ¨ç†ä¼šè¯
    network_file = 'ICLabelNet.onnx'
    session = ort.InferenceSession(network_file)
    
    # 2. æ ¼å¼åŒ–è¾“å…¥
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    topo = np.transpose(topo, (3,2,0,1)).astype(np.float32)
    psd = np.transpose(psd, (3,2,0,1)).astype(np.float32)
    autocorr = np.transpose(autocorr, (3,2,0,1)).astype(np.float32)
    
    # 3. æ¨ç†
    labels = session.run(
        None,  # è¾“å‡ºåç§° (None = æ‰€æœ‰è¾“å‡º)
        {
            'topo': topo,
            'psds': psd,
            'autocorr': autocorr
        }
    )
    
    # 4. è¿”å›ç¬¬ä¸€ä¸ªè¾“å‡º
    return labels[0]
```

**ç‰¹ç‚¹**:
- âœ… è½»é‡çº§ (~50MB)
- âœ… CPU ä¼˜åŒ–è‰¯å¥½
- âš ï¸ ç¨æ…¢äº PyTorch (~20%)

---

## ç†è®ºä¸Šå¯æ·»åŠ çš„åç«¯

### å¯¹æ¯”è¡¨

| åç«¯ | å¯è¡Œæ€§ | æ¨¡å‹è½¬æ¢ | å·¥ä½œé‡ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|-------|---------|-------|------|------|
| **TensorFlow** | âœ… é«˜ | ONNXâ†’TF | ä¸­ç­‰ | ç”Ÿæ€æˆç†Ÿï¼ŒKeras API å‹å¥½ | ä¾èµ–å¤§ |
| **TensorRT** | âœ… é«˜ | ONNXâ†’TRT | è¾ƒé«˜ | NVIDIA GPU æè‡´æ€§èƒ½ | ä»… GPUï¼Œå¤æ‚ |
| **OpenVINO** | âœ… é«˜ | ONNXâ†’IR | ä¸­ç­‰ | Intel CPU/GPU ä¼˜åŒ– | å¹³å°é™åˆ¶ |
| **TensorFlow Lite** | âœ… é«˜ | TFâ†’TFLite | ä¸­ç­‰ | ç§»åŠ¨/åµŒå…¥å¼ | åŠŸèƒ½å—é™ |
| **Core ML** | âš ï¸ ä¸­ | ONNXâ†’mlmodel | è¾ƒé«˜ | iOS/macOS åŸç”Ÿ | ä»…è‹¹æœå¹³å° |
| **Burn (Rust)** | âš ï¸ ä¸­ | æ‰‹åŠ¨å®ç° | é«˜ | æ—  Python ä¾èµ–ï¼Œå¿« | ç”Ÿæ€ä¸æˆç†Ÿ |
| **Candle (Rust)** | âœ… é«˜ | PyTorchâ†’Candle | ä¸­ç­‰ | Rust åŸç”Ÿï¼ŒGPU æ”¯æŒ | æ–°é¡¹ç›®ï¼Œç¨³å®šæ€§ |
| **MLC-LLM** | âš ï¸ ä½ | ä¸é€‚ç”¨ | - | - | é’ˆå¯¹ LLM |

---

### æ·»åŠ  TensorFlow åç«¯ç¤ºä¾‹

#### æ­¥éª¤ 1: æ¨¡å‹è½¬æ¢

```python
# æ–¹æ³• 1: ONNX â†’ TensorFlow
import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load('ICLabelNet.onnx')
tf_rep = prepare(onnx_model)
tf_rep.export_graph('ICLabelNet_tf')

# æ–¹æ³• 2: PyTorch â†’ ONNX â†’ TensorFlow (æ›´å¯é )
import torch
model = ICLabelNet()
model.load_state_dict(torch.load('ICLabelNet.pt'))

dummy_topo = torch.randn(1, 1, 32, 32)
dummy_psd = torch.randn(1, 1, 1, 100)
dummy_autocorr = torch.randn(1, 1, 1, 100)

torch.onnx.export(
    model,
    (dummy_topo, dummy_psd, dummy_autocorr),
    'ICLabelNet.onnx',
    input_names=['topo', 'psds', 'autocorr'],
    output_names=['labels'],
    dynamic_axes={
        'topo': {0: 'batch'},
        'psds': {0: 'batch'},
        'autocorr': {0: 'batch'}
    }
)
```

#### æ­¥éª¤ 2: æ¨ç†å®ç°

```python
# mne_icalabel/iclabel/network/tensorflow.py
import tensorflow as tf
import numpy as np

def _run_iclabel_tf(images, psds, autocorr):
    """TensorFlow æ¨ç†"""
    # åŠ è½½æ¨¡å‹
    model = tf.saved_model.load('ICLabelNet_tf')
    infer = model.signatures['serving_default']
    
    # æ ¼å¼åŒ–
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    topo = tf.constant(np.transpose(topo, (3,2,0,1)), dtype=tf.float32)
    psd = tf.constant(np.transpose(psd, (3,2,0,1)), dtype=tf.float32)
    autocorr = tf.constant(np.transpose(autocorr, (3,2,0,1)), dtype=tf.float32)
    
    # æ¨ç†
    outputs = infer(topo=topo, psds=psd, autocorr=autocorr)
    
    return outputs['labels'].numpy()
```

---

### æ·»åŠ  OpenVINO åç«¯ç¤ºä¾‹

```bash
# è½¬æ¢æ¨¡å‹
mo --input_model ICLabelNet.onnx \
   --output_dir openvino_model \
   --data_type FP32
```

```python
# mne_icalabel/iclabel/network/openvino.py
from openvino.runtime import Core

def _run_iclabel_openvino(images, psds, autocorr):
    """OpenVINO æ¨ç† (Intel ä¼˜åŒ–)"""
    # åˆå§‹åŒ–
    ie = Core()
    model = ie.read_model('ICLabelNet.xml')
    compiled = ie.compile_model(model, 'CPU')
    
    # æ ¼å¼åŒ–
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    topo = np.transpose(topo, (3,2,0,1)).astype(np.float32)
    psd = np.transpose(psd, (3,2,0,1)).astype(np.float32)
    autocorr = np.transpose(autocorr, (3,2,0,1)).astype(np.float32)
    
    # æ¨ç†
    results = compiled([topo, psd, autocorr])
    
    return results[0]
```

---

## Rust åç«¯å¯è¡Œæ€§åˆ†æ

### Burn vs Candle å¯¹æ¯”

| ç‰¹æ€§ | Burn | Candle |
|------|------|--------|
| **å¼€å‘è€…** | tracel-ai | Hugging Face |
| **æˆç†Ÿåº¦** | âš ï¸ æ—©æœŸ (v0.13) | âš ï¸ æ–°é¡¹ç›® (2023) |
| **GPU æ”¯æŒ** | âœ… CUDA, Metal, WebGPU | âœ… CUDA, Metal |
| **æ¨¡å‹å¯¼å…¥** | âŒ éœ€æ‰‹åŠ¨å®ç° | âœ… PyTorch æƒé‡ |
| **Python ç»‘å®š** | âš ï¸ æœ‰é™ (PyO3) | âœ… å®˜æ–¹æ”¯æŒ |
| **ç”Ÿæ€** | å° | æˆé•¿ä¸­ |

---

### Candle åç«¯å®ç° (æ¨è)

**ä¸ºä»€ä¹ˆé€‰ Candle**:
1. âœ… å¯ä»¥ç›´æ¥åŠ è½½ PyTorch æƒé‡ (`.pt` æ–‡ä»¶)
2. âœ… æœ‰ Python ç»‘å®š (`candle-pyo3`)
3. âœ… GPU æ”¯æŒå®Œå–„
4. âœ… Hugging Face ç»´æŠ¤ï¼Œæ›´æ–°æ´»è·ƒ

#### æ­¥éª¤ 1: å®‰è£… Candle

```bash
# Rust ä¾§
cargo add candle-core candle-nn

# Python ç»‘å®š (å¦‚æœæœ‰)
pip install candle-pyo3
```

#### æ­¥éª¤ 2: å®ç° Rust æ¨¡å‹

```rust
// src/iclabel_net.rs
use candle_core::{Tensor, Device, DType};
use candle_nn::{Conv2d, Conv2dConfig, Module, VarBuilder, ops::softmax};

struct ICLabelNet {
    // æ‹“æ‰‘åˆ†æ”¯
    img_conv1: Conv2d,
    img_conv2: Conv2d,
    img_conv3: Conv2d,
    
    // PSD åˆ†æ”¯
    psd_conv1: Conv2d,
    psd_conv2: Conv2d,
    psd_conv3: Conv2d,
    
    // Autocorr åˆ†æ”¯
    autocorr_conv1: Conv2d,
    autocorr_conv2: Conv2d,
    autocorr_conv3: Conv2d,
    
    // èåˆå±‚
    final_conv: Conv2d,
}

impl ICLabelNet {
    fn new(vb: VarBuilder) -> Result<Self> {
        // æ‹“æ‰‘åˆ†æ”¯
        let img_conv1 = candle_nn::conv2d(
            1, 128, 4,
            Conv2dConfig {
                stride: 2,
                padding: 1,
                ..Default::default()
            },
            vb.pp("img_conv.0")
        )?;
        
        // ... (ç±»ä¼¼å®šä¹‰å…¶ä»–å±‚)
        
        Ok(Self {
            img_conv1,
            // ...
        })
    }
    
    fn forward(&self, topo: &Tensor, psd: &Tensor, autocorr: &Tensor) 
        -> Result<Tensor> 
    {
        // æ‹“æ‰‘åˆ†æ”¯
        let img = self.img_conv1.forward(topo)?;
        let img = img.relu()?;  // LeakyReLU ç®€åŒ–
        let img = self.img_conv2.forward(&img)?;
        let img = img.relu()?;
        let img = self.img_conv3.forward(&img)?;
        let img = img.relu()?;
        
        // PSD åˆ†æ”¯
        let psd_out = self.psd_conv1.forward(psd)?;
        let psd_out = psd_out.relu()?;
        let psd_out = self.psd_conv2.forward(&psd_out)?;
        let psd_out = psd_out.relu()?;
        let psd_out = self.psd_conv3.forward(&psd_out)?;
        
        // Autocorr åˆ†æ”¯ (åŒç†)
        // ...
        
        // é‡å¡‘å’Œæ‹¼æ¥
        let psd_reshaped = psd_out.permute((0, 3, 1, 2))?.repeat((1, 1, 4, 4))?;
        let autocorr_reshaped = autocorr_out.permute((0, 3, 1, 2))?.repeat((1, 1, 4, 4))?;
        
        let concat = Tensor::cat(&[img, psd_reshaped, autocorr_reshaped], 1)?;
        
        // æœ€ç»ˆå±‚
        let out = self.final_conv.forward(&concat)?;
        let out = out.squeeze(2)?.squeeze(2)?;
        let out = softmax(&out, 1)?;
        
        Ok(out)
    }
}

// åŠ è½½ PyTorch æƒé‡
fn load_model(device: &Device) -> Result<ICLabelNet> {
    let weights = candle_core::safetensors::load(
        "ICLabelNet.safetensors",  // éœ€ä» .pt è½¬æ¢
        device
    )?;
    let vb = VarBuilder::from_tensors(weights, DType::F32, device);
    ICLabelNet::new(vb)
}
```

#### æ­¥éª¤ 3: Python ç»‘å®š

```python
# mne_icalabel/iclabel/network/candle.py
import numpy as np
from candle_pyo3 import ICLabelNet  # Rust ç¼–è¯‘çš„ Python æ‰©å±•

def _run_iclabel_candle(images, psds, autocorr):
    """Candle (Rust) æ¨ç†"""
    # åŠ è½½æ¨¡å‹
    model = ICLabelNet.from_pretrained('ICLabelNet.safetensors')
    
    # æ ¼å¼åŒ–
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    topo = np.transpose(topo, (3,2,0,1)).astype(np.float32)
    psd = np.transpose(psd, (3,2,0,1)).astype(np.float32)
    autocorr = np.transpose(autocorr, (3,2,0,1)).astype(np.float32)
    
    # æ¨ç† (è°ƒç”¨ Rust)
    labels = model.forward(topo, psd, autocorr)
    
    return labels
```

#### æ­¥éª¤ 4: æƒé‡è½¬æ¢

```python
# PyTorch .pt â†’ Safetensors (Candle æ”¯æŒæ ¼å¼)
import torch
from safetensors.torch import save_file

# åŠ è½½ PyTorch æƒé‡
state_dict = torch.load('ICLabelNet.pt', map_location='cpu')

# ä¿å­˜ä¸º Safetensors
save_file(state_dict, 'ICLabelNet.safetensors')
```

---

### Burn åç«¯å®ç° (ä¸æ¨è)

**ä¸ºä»€ä¹ˆä¸æ¨è Burn**:
- âŒ ä¸èƒ½ç›´æ¥åŠ è½½ PyTorch æƒé‡
- âŒ éœ€è¦æ‰‹åŠ¨å®ç°æ‰€æœ‰å±‚
- âŒ Python ç»‘å®šä¸å®Œå–„
- âš ï¸ ç”Ÿæ€å¤ªæ–°ï¼Œæ–‡æ¡£å°‘

**å¦‚æœçœŸè¦ç”¨ Burn**:

```rust
// éœ€è¦æ‰‹åŠ¨å®šä¹‰å¹¶è®­ç»ƒæ¨¡å‹
use burn::{
    nn::{conv::Conv2d, Linear},
    tensor::{Tensor, backend::Backend},
};

// å®Œå…¨ä»å¤´å®ç°ï¼Œä¸èƒ½å¤ç”¨å·²æœ‰æƒé‡
// å·¥ä½œé‡æå¤§ï¼Œä¸åˆ‡å®é™…
```

---

## åç«¯é€‰æ‹©å†³ç­–æ ‘

```
å¼€å§‹
 â”‚
 â”œâ”€ éœ€è¦ GPU æè‡´æ€§èƒ½? â”€â”€YESâ”€â†’ TensorRT (NVIDIA)
 â”‚                              æˆ– OpenVINO (Intel)
 â”‚
 â”œâ”€ éœ€è¦åµŒå…¥å¼/ç§»åŠ¨? â”€â”€YESâ”€â†’ TensorFlow Lite
 â”‚                           æˆ– Core ML (iOS)
 â”‚
 â”œâ”€ éœ€è¦ Rust åŸç”Ÿ? â”€â”€YESâ”€â†’ Candle âœ…
 â”‚  (æ—  Python ä¾èµ–)          (ä¸æ¨è Burn)
 â”‚
 â”œâ”€ éœ€è¦è·¨å¹³å° CPU? â”€â”€YESâ”€â†’ ONNX Runtime âœ… (å·²æœ‰)
 â”‚
 â””â”€ é»˜è®¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ PyTorch âœ… (å·²æœ‰)
```

---

## æ·»åŠ æ–°åç«¯çš„é€šç”¨æµç¨‹

### å®Œæ•´ Checklist

#### 1. æ¨¡å‹è½¬æ¢ âœ…

```bash
# ç¤ºä¾‹: PyTorch â†’ ONNX â†’ ç›®æ ‡æ ¼å¼
python convert_to_onnx.py
onnx-tool optimize ICLabelNet.onnx -o ICLabelNet_opt.onnx
# ç„¶åè½¬ä¸ºç›®æ ‡æ ¼å¼ (TF, TRT, OpenVINO, etc.)
```

#### 2. å®ç°æ¨ç†å‡½æ•° âœ…

```python
# mne_icalabel/iclabel/network/your_backend.py

from .utils import _format_input
import numpy as np

def _run_iclabel(images, psds, autocorr):
    """
    ä½ çš„åç«¯æ¨ç†å®ç°
    
    å¿…é¡»:
    1. è°ƒç”¨ _format_input() è¿›è¡Œæ•°æ®å¢å¼º
    2. Transpose ä¸º batch-first (NCHW)
    3. æ¨ç†è¿”å› (n_components*4, 7)
    4. MNE-ICALabel ä¼šè‡ªåŠ¨å¤„ç†åç»­å¹³å‡
    """
    # 1. æ•°æ®å¢å¼º
    topo, psd, autocorr = _format_input(images, psds, autocorr)
    
    # 2. Transpose
    topo = np.transpose(topo, (3, 2, 0, 1))
    psd = np.transpose(psd, (3, 2, 0, 1))
    autocorr = np.transpose(autocorr, (3, 2, 0, 1))
    
    # 3. è½¬ä¸ºä½ çš„åç«¯æ ¼å¼ (TF Tensor, TRT Input, etc.)
    topo_backend = convert_to_backend_format(topo)
    psd_backend = convert_to_backend_format(psd)
    autocorr_backend = convert_to_backend_format(autocorr)
    
    # 4. æ¨ç†
    labels = model.predict({
        'topo': topo_backend,
        'psds': psd_backend,
        'autocorr': autocorr_backend
    })
    
    # 5. è¿”å› NumPy
    return labels
```

#### 3. ä¿®æ”¹åç«¯é€‰æ‹©é€»è¾‘ âœ…

```python
# mne_icalabel/iclabel/network/__init__.py

def run_iclabel(images, psds, autocorr, backend=None):
    _check_option("backend", backend, 
                  (None, "torch", "onnx", "your_backend"))  # â† æ·»åŠ 
    
    if backend == "your_backend":
        import_optional_dependency("your_framework", raise_error=True)
        from .your_backend import _run_iclabel
        return _run_iclabel(images, psds, autocorr)
    
    # ... åŸæœ‰é€»è¾‘
```

#### 4. ç¼–å†™å•å…ƒæµ‹è¯• âœ…

```python
# mne_icalabel/iclabel/tests/test_backends.py

@requires_module("your_framework")
def test_your_backend():
    # åŠ è½½æµ‹è¯•æ•°æ®
    raw, ica = load_test_data()
    
    # PyTorch åŸºå‡†
    labels_torch = iclabel_label_components(
        raw, ica, backend='torch', inplace=False
    )
    
    # ä½ çš„åç«¯
    labels_yours = iclabel_label_components(
        raw, ica, backend='your_backend', inplace=False
    )
    
    # æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥
    np.testing.assert_allclose(
        labels_torch, labels_yours,
        rtol=1e-5, atol=1e-6
    )
```

#### 5. æ›´æ–°æ–‡æ¡£ âœ…

```python
# mne_icalabel/iclabel/label_components.py

def iclabel_label_components(inst, ica, backend=None):
    """
    Parameters
    ----------
    backend : None | 'torch' | 'onnx' | 'your_backend'
        Backend to use. If None, auto-selects.
        
        - 'torch': PyTorch (fastest on GPU)
        - 'onnx': ONNX Runtime (lightweight)
        - 'your_backend': Your new backend (describe here)
    """
```

---

## æ€§èƒ½åŸºå‡† (é¢„ä¼°)

| åç«¯ | CPU é€Ÿåº¦ | GPU é€Ÿåº¦ | å†…å­˜å ç”¨ | å®‰è£…å¤§å° |
|------|---------|---------|---------|---------|
| **PyTorch** | ~150ms | ~50ms | ~500MB | ~1GB |
| **ONNX** | ~200ms | ~80ms | ~100MB | ~50MB |
| **TensorFlow** | ~180ms | ~60ms | ~400MB | ~500MB |
| **TensorRT** | N/A | ~30ms âš¡ | ~300MB | ~500MB |
| **OpenVINO** | ~120ms ğŸš€ | ~70ms | ~150MB | ~200MB |
| **Candle (Rust)** | ~160ms | ~55ms | ~80MB ğŸ’¾ | ~30MB |
| **TFLite** | ~250ms | N/A | ~50MB ğŸ’¾ | ~20MB |

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ¨¡å‹è¾“å…¥ = 3 ç§ç‰¹å¾**
   - æ‹“æ‰‘å›¾: (32, 32, 1, n_comp) ç©ºé—´åˆ†å¸ƒ
   - PSD: (1, 100, 1, n_comp) é¢‘ç‡ç‰¹æ€§
   - è‡ªç›¸å…³: (1, 100, 1, n_comp) æ—¶é—´è§„å¾‹

2. **æ•°æ®å¢å¼º = x4 æ ·æœ¬**
   - æ‹“æ‰‘å›¾ç¿»è½¬/å–å 4 ç§å˜æ¢
   - PSD/è‡ªç›¸å…³ç®€å•å¤åˆ¶
   - æ¨ç†åå¹³å‡ç»“æœ

3. **ç°æœ‰åç«¯ = PyTorch + ONNX**
   - PyTorch: GPU å¿«ï¼Œä¾èµ–å¤§
   - ONNX: CPU å¥½ï¼Œè½»é‡çº§

4. **å¯æ·»åŠ åç«¯**
   - âœ… **é«˜å¯è¡Œæ€§**: TensorFlow, TensorRT, OpenVINO, Candle
   - âš ï¸ **ä¸­ç­‰å¯è¡Œæ€§**: TFLite, Core ML
   - âŒ **ä¸æ¨è**: Burn (ç”Ÿæ€ä¸æˆç†Ÿ)

5. **Rust åç«¯æ¨è**
   - **Candle** âœ…: å¯åŠ è½½ PyTorch æƒé‡ï¼ŒGPU æ”¯æŒå¥½
   - **Burn** âŒ: éœ€ä»å¤´å®ç°ï¼Œå·¥ä½œé‡å¤§

---

**ç›¸å…³æ–‡æ¡£**:
- [MNE-ICALabel è‡ªåŠ¨åˆ†ç±»æŒ‡å—](mne-icalabel-guide.md)
- [MNE ç¦»çº¿å¤„ç†æŒ‡å—](mne-offline-processing.md)
- [MNE å®æ—¶å¤„ç†æŒ‡å—](mne-realtime-processing.md)
